
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>GLM: Robust Regression with Outlier Detection &#8212; PyMC3 3.8 documentation</title>
    <link rel="stylesheet" href="../_static/semantic-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/semantic-ui@2.4.2/dist/semantic.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/default.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/language_data.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <script type="text/javascript" src="../_static/highlight.min.js"></script>
    <script type="text/javascript" src="../_static/semantic.min.js"></script>
    <link rel="shortcut icon" href="../_static/PyMC3.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
<script>hljs.initHighlightingOnLoad();</script>
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">



  </head><body>
<div class="ui vertical center aligned">

    <div class="ui container">
        <div class="ui large secondary pointing menu">
            <a class="item" href="/">
                <img class="ui bottom aligned tiny image" src="https://cdn.rawgit.com/pymc-devs/pymc3/master/docs/logos/svg/PyMC3_banner.svg" />
            </a>
             <a href="../nb_tutorials/index.html" class="item">Tutorials</a> <a href="../nb_examples/index.html" class="item">Examples</a> <a href="../learn.html" class="item">Books + Videos</a> <a href="../api.html" class="item">API</a> <a href="../developer_guide.html" class="item">Developer Guide</a> <a href="../history.html" class="item">About PyMC3</a>
            
            <div class="right menu">
                <div class="item">
                    <form class="ui icon input" action="../search.html" method="get">
                        <input type="text" placeholder="Search..." name="q" />
                        <i class="search link icon"></i>
                    </form>
                </div>
                <a class="item" href="https://github.com/pymc-devs/pymc3"><i class="github blue icon large"></i></a>
            </div>
        </div>
    </div>
    
</div>

<div class="ui container" role="main">
    

    <div class="ui vertical segment">
        
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container,
div.nbinput.container div.prompt,
div.nbinput.container div.input_area,
div.nbinput.container div[class*=highlight],
div.nbinput.container div[class*=highlight] pre,
div.nboutput.container,
div.nboutput.container div.prompt,
div.nboutput.container div.output_area,
div.nboutput.container div[class*=highlight],
div.nboutput.container div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    min-width: 7ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    background: #f5f5f5;
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
</style>
<div class="section" id="GLM:-Robust-Regression-with-Outlier-Detection">
<h1>GLM: Robust Regression with Outlier Detection<a class="headerlink" href="#GLM:-Robust-Regression-with-Outlier-Detection" title="Permalink to this headline">¶</a></h1>
<p><strong>A minimal reproducable example of Robust Regression with Outlier Detection using Hogg 2010 Signal vs Noise method.</strong></p>
<ul class="simple">
<li><p>This is a complementary approach to the Student-T robust regression as illustrated in [Thomas Wiecki’s notebook]((GLM-robust.ipynb), that approach is also compared here.</p></li>
<li><p>This model returns a robust estimate of linear coefficients and an indication of which datapoints (if any) are outliers.</p></li>
<li><p>The likelihood evaluation is essentially a copy of eqn 17 in “Data analysis recipes: Fitting a model to data” - <a class="reference external" href="http://arxiv.org/abs/1008.4686">Hogg 2010</a>.</p></li>
<li><p>The model is adapted specifically from Jake Vanderplas’ <a class="reference external" href="http://www.astroml.org/book_figures/chapter8/fig_outlier_rejection.html">implementation</a> (3rd model tested).</p></li>
<li><p>The dataset is tiny and hardcoded into this Notebook. It contains errors in both the x and y, but we will deal here with only errors in y.</p></li>
</ul>
<p><strong>Note:</strong></p>
<ul class="simple">
<li><p>Python 3.4 project using latest available <a class="reference external" href="https://github.com/pymc-devs/pymc3">PyMC3</a></p></li>
<li><p>Developed using <a class="reference external" href="https://www.continuum.io/downloads">ContinuumIO Anaconda</a> distribution on a Macbook Pro 3GHz i7, 16GB RAM, OSX 10.10.5.</p></li>
<li><p>During development I’ve found that 3 data points are always indicated as outliers, but the remaining ordering of datapoints by decreasing outlier-hood is slightly unstable between runs: the posterior surface appears to have a small number of solutions with similar probability.</p></li>
<li><p>Finally, if runs become unstable or Theano throws weird errors, try clearing the cache <code class="docutils literal notranslate"><span class="pre">$&gt;</span> <span class="pre">theano-cache</span> <span class="pre">clear</span></code> and rerunning the notebook.</p></li>
</ul>
<p><strong>Package Requirements (shown as a conda-env YAML):</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$&gt; less conda_env_pymc3_examples.yml

name: pymc3_examples
    channels:
      - defaults
    dependencies:
      - python=3.4
      - ipython
      - ipython-notebook
      - ipython-qtconsole
      - numpy
      - scipy
      - matplotlib
      - pandas
      - seaborn
      - patsy
      - pip

$&gt; conda env create --file conda_env_pymc3_examples.yml

$&gt; source activate pymc3_examples

$&gt; pip install --process-dependency-links git+https://github.com/pymc-devs/pymc3
</pre></div>
</div>
<div class="section" id="Setup">
<h2>Setup<a class="headerlink" href="#Setup" title="Permalink to this headline">¶</a></h2>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline

<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">optimize</span>
<span class="kn">import</span> <span class="nn">pymc3</span> <span class="k">as</span> <span class="nn">pm</span>
<span class="kn">import</span> <span class="nn">theano</span>
<span class="kn">import</span> <span class="nn">theano.tensor</span> <span class="k">as</span> <span class="nn">tt</span>

<span class="c1"># configure some basic options</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">&quot;darkgrid&quot;</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s2">&quot;muted&quot;</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;display.notebook_repr_html&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">8</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Running on PyMC3 v</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pm</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Running on PyMC3 v3.6
</pre></div></div>
</div>
<div class="section" id="Load-and-Prepare-Data">
<h3>Load and Prepare Data<a class="headerlink" href="#Load-and-Prepare-Data" title="Permalink to this headline">¶</a></h3>
<p>We’ll use the Hogg 2010 data available at <a class="reference external" href="https://github.com/astroML/astroML/blob/master/astroML/datasets/hogg2010test.py">https://github.com/astroML/astroML/blob/master/astroML/datasets/hogg2010test.py</a></p>
<p>It’s a very small dataset so for convenience, it’s hardcoded below</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1">#### cut &amp; pasted directly from the fetch_hogg2010test() function</span>
<span class="c1">## identical to the original dataset as hardcoded in the Hogg 2010 paper</span>

<span class="n">dfhogg</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">201</span><span class="p">,</span> <span class="mi">592</span><span class="p">,</span> <span class="mi">61</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.84</span><span class="p">],</span>
                                 <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">244</span><span class="p">,</span> <span class="mi">401</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mf">0.31</span><span class="p">],</span>
                                 <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">47</span><span class="p">,</span> <span class="mi">583</span><span class="p">,</span> <span class="mi">38</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mf">0.64</span><span class="p">],</span>
                                 <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">287</span><span class="p">,</span> <span class="mi">402</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.27</span><span class="p">],</span>
                                 <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">203</span><span class="p">,</span> <span class="mi">495</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.33</span><span class="p">],</span>
                                 <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">58</span><span class="p">,</span> <span class="mi">173</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mf">0.67</span><span class="p">],</span>
                                 <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">210</span><span class="p">,</span> <span class="mi">479</span><span class="p">,</span> <span class="mi">27</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.02</span><span class="p">],</span>
                                 <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">202</span><span class="p">,</span> <span class="mi">504</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.05</span><span class="p">],</span>
                                 <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">198</span><span class="p">,</span> <span class="mi">510</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.84</span><span class="p">],</span>
                                 <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">158</span><span class="p">,</span> <span class="mi">416</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.69</span><span class="p">],</span>
                                 <span class="p">[</span><span class="mi">11</span><span class="p">,</span> <span class="mi">165</span><span class="p">,</span> <span class="mi">393</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.30</span><span class="p">],</span>
                                 <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">201</span><span class="p">,</span> <span class="mi">442</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.46</span><span class="p">],</span>
                                 <span class="p">[</span><span class="mi">13</span><span class="p">,</span> <span class="mi">157</span><span class="p">,</span> <span class="mi">317</span><span class="p">,</span> <span class="mi">52</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.03</span><span class="p">],</span>
                                 <span class="p">[</span><span class="mi">14</span><span class="p">,</span> <span class="mi">131</span><span class="p">,</span> <span class="mi">311</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mf">0.50</span><span class="p">],</span>
                                 <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">166</span><span class="p">,</span> <span class="mi">400</span><span class="p">,</span> <span class="mi">34</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mf">0.73</span><span class="p">],</span>
                                 <span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">160</span><span class="p">,</span> <span class="mi">337</span><span class="p">,</span> <span class="mi">31</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.52</span><span class="p">],</span>
                                 <span class="p">[</span><span class="mi">17</span><span class="p">,</span> <span class="mi">186</span><span class="p">,</span> <span class="mi">423</span><span class="p">,</span> <span class="mi">42</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mf">0.90</span><span class="p">],</span>
                                 <span class="p">[</span><span class="mi">18</span><span class="p">,</span> <span class="mi">125</span><span class="p">,</span> <span class="mi">334</span><span class="p">,</span> <span class="mi">26</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mf">0.40</span><span class="p">],</span>
                                 <span class="p">[</span><span class="mi">19</span><span class="p">,</span> <span class="mi">218</span><span class="p">,</span> <span class="mi">533</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.78</span><span class="p">],</span>
                                 <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">146</span><span class="p">,</span> <span class="mi">344</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.56</span><span class="p">]]),</span>
                   <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">,</span><span class="s1">&#39;x&#39;</span><span class="p">,</span><span class="s1">&#39;y&#39;</span><span class="p">,</span><span class="s1">&#39;sigma_y&#39;</span><span class="p">,</span><span class="s1">&#39;sigma_x&#39;</span><span class="p">,</span><span class="s1">&#39;rho_xy&#39;</span><span class="p">])</span>


<span class="c1">## for convenience zero-base the &#39;id&#39; and use as index</span>
<span class="n">dfhogg</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dfhogg</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span>
<span class="n">dfhogg</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s1">&#39;id&#39;</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1">## standardize (mean center and divide by 1 sd)</span>
<span class="n">dfhoggs</span> <span class="o">=</span> <span class="p">(</span><span class="n">dfhogg</span><span class="p">[[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span><span class="s1">&#39;y&#39;</span><span class="p">]]</span> <span class="o">-</span> <span class="n">dfhogg</span><span class="p">[[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span><span class="s1">&#39;y&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span> <span class="o">/</span> <span class="n">dfhogg</span><span class="p">[[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span><span class="s1">&#39;y&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">dfhoggs</span><span class="p">[</span><span class="s1">&#39;sigma_y&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dfhogg</span><span class="p">[</span><span class="s1">&#39;sigma_y&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="n">dfhogg</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">dfhoggs</span><span class="p">[</span><span class="s1">&#39;sigma_x&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dfhogg</span><span class="p">[</span><span class="s1">&#39;sigma_x&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="n">dfhogg</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1">## create xlims ylims for plotting</span>
<span class="n">xlims</span> <span class="o">=</span> <span class="p">(</span><span class="n">dfhoggs</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">ptp</span><span class="p">(</span><span class="n">dfhoggs</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span><span class="o">/</span><span class="mi">5</span>
                 <span class="p">,</span><span class="n">dfhoggs</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">ptp</span><span class="p">(</span><span class="n">dfhoggs</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span><span class="o">/</span><span class="mi">5</span><span class="p">)</span>
<span class="n">ylims</span> <span class="o">=</span> <span class="p">(</span><span class="n">dfhoggs</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">ptp</span><span class="p">(</span><span class="n">dfhoggs</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span><span class="o">/</span><span class="mi">5</span>
                 <span class="p">,</span><span class="n">dfhoggs</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">ptp</span><span class="p">(</span><span class="n">dfhoggs</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span><span class="o">/</span><span class="mi">5</span><span class="p">)</span>

<span class="c1">## scatterplot the standardized data</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">FacetGrid</span><span class="p">(</span><span class="n">dfhoggs</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">errorbar</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;sigma_y&#39;</span><span class="p">,</span> <span class="s1">&#39;sigma_x&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">ylims</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">xlims</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">top</span><span class="o">=</span><span class="mf">0.92</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Scatterplot of Hogg 2010 dataset after standardization&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_GLM-robust-with-outlier-detection_6_0.png" src="../_images/notebooks_GLM-robust-with-outlier-detection_6_0.png" />
</div>
</div>
<p><strong>Observe</strong>:</p>
<ul class="simple">
<li><p>Even judging just by eye, you can see these datapoints mostly fall on / around a straight line with positive gradient</p></li>
<li><p>It looks like a few of the datapoints may be outliers from such a line</p></li>
</ul>
</div>
</div>
<div class="section" id="Create-Conventional-OLS-Model">
<h2>Create Conventional OLS Model<a class="headerlink" href="#Create-Conventional-OLS-Model" title="Permalink to this headline">¶</a></h2>
<p>The <em>linear model</em> is really simple and conventional:</p>
<div class="math notranslate nohighlight">
\[\bf{y} = \beta^{T} \bf{X} + \bf{\sigma}\]</div>
<p>where:</p>
<div class="line-block">
<div class="line"><span class="math notranslate nohighlight">\(\beta\)</span> = coefs = <span class="math notranslate nohighlight">\(\{1, \beta_{j \in X_{j}}\}\)</span></div>
<div class="line"><span class="math notranslate nohighlight">\(\sigma\)</span> = the measured error in <span class="math notranslate nohighlight">\(y\)</span> in the dataset <code class="docutils literal notranslate"><span class="pre">sigma_y</span></code></div>
</div>
<div class="section" id="Define-model">
<h3>Define model<a class="headerlink" href="#Define-model" title="Permalink to this headline">¶</a></h3>
<p><strong>NOTE:</strong> + We’re using a simple linear OLS model with Normally distributed priors so that it behaves like a ridge regression</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">mdl_ols</span><span class="p">:</span>

    <span class="c1">## Define weakly informative Normal priors to give Ridge regression</span>
    <span class="n">b0</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;b0_intercept&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;b1_slope&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1">## Define linear model</span>
    <span class="n">yest</span> <span class="o">=</span> <span class="n">b0</span> <span class="o">+</span> <span class="n">b1</span> <span class="o">*</span> <span class="n">dfhoggs</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span>

    <span class="c1">## Use y error from dataset, convert into theano variable</span>
    <span class="n">sigma_y</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">dfhoggs</span><span class="p">[</span><span class="s1">&#39;sigma_y&#39;</span><span class="p">],</span>
                            <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;sigma_y&#39;</span><span class="p">)</span>

    <span class="c1">## Define Normal likelihood</span>
    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;likelihood&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">yest</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma_y</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">dfhoggs</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Sample">
<h3>Sample<a class="headerlink" href="#Sample" title="Permalink to this headline">¶</a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">mdl_ols</span><span class="p">:</span>
    <span class="c1">## take samples</span>
    <span class="n">traces_ols</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (2 chains in 2 jobs)
NUTS: [b1_slope, b0_intercept]
Sampling 2 chains: 100%|██████████| 2000/2000 [00:00&lt;00:00, 2023.86draws/s]
</pre></div></div>
</div>
</div>
<div class="section" id="View-Traces">
<h3>View Traces<a class="headerlink" href="#View-Traces" title="Permalink to this headline">¶</a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">_</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">traceplot</span><span class="p">(</span><span class="n">traces_ols</span><span class="p">,</span>
                 <span class="n">lines</span><span class="o">=</span><span class="nb">tuple</span><span class="p">([(</span><span class="n">k</span><span class="p">,</span> <span class="p">{},</span> <span class="n">v</span><span class="p">[</span><span class="s1">&#39;mean&#39;</span><span class="p">])</span>
                              <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">pm</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">traces_ols</span><span class="p">)</span><span class="o">.</span><span class="n">iterrows</span><span class="p">()]))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_GLM-robust-with-outlier-detection_15_0.png" src="../_images/notebooks_GLM-robust-with-outlier-detection_15_0.png" />
</div>
</div>
<p><strong>NOTE:</strong> We’ll illustrate this OLS fit and compare to the datapoints in the final plot</p>
</div>
</div>
<div class="section" id="Create-Robust-Model:-Student-T-Method">
<h2>Create Robust Model: Student-T Method<a class="headerlink" href="#Create-Robust-Model:-Student-T-Method" title="Permalink to this headline">¶</a></h2>
<p>I’ve added this brief section in order to directly compare the Student-T based method exampled in <a class="reference internal" href="GLM-robust.html"><span class="doc">Thomas Wiecki’s notebook</span></a>.</p>
<p>Instead of using a Normal distribution for the likelihood, we use a Student-T, which has fatter tails. In theory this allows outliers to have a smaller mean square error in the likelihood, and thus have less influence on the regression estimation. This method does not produce inlier / outlier flags but is simpler and faster to run than the Signal Vs Noise model below, so a comparison seems worthwhile.</p>
<p><strong>Note:</strong> we’ll constrain the Student-T ‘degrees of freedom’ parameter <code class="docutils literal notranslate"><span class="pre">nu</span></code> to be an integer, but otherwise leave it as just another stochastic to be inferred: no need for prior knowledge.</p>
<div class="section" id="Define-Model">
<h3>Define Model<a class="headerlink" href="#Define-Model" title="Permalink to this headline">¶</a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">mdl_studentt</span><span class="p">:</span>

    <span class="c1">## Define weakly informative Normal priors to give Ridge regression</span>
    <span class="n">b0</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;b0_intercept&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;b1_slope&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1">## Define linear model</span>
    <span class="n">yest</span> <span class="o">=</span> <span class="n">b0</span> <span class="o">+</span> <span class="n">b1</span> <span class="o">*</span> <span class="n">dfhoggs</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span>

    <span class="c1">## Use y error from dataset, convert into theano variable</span>
    <span class="n">sigma_y</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">dfhoggs</span><span class="p">[</span><span class="s1">&#39;sigma_y&#39;</span><span class="p">],</span>
                            <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;sigma_y&#39;</span><span class="p">)</span>

    <span class="c1">## define prior for Student T degrees of freedom</span>
    <span class="n">nu</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s1">&#39;nu&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

    <span class="c1">## Define Student T likelihood</span>
    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">StudentT</span><span class="p">(</span><span class="s1">&#39;likelihood&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">yest</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma_y</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">nu</span><span class="p">,</span>
                             <span class="n">observed</span><span class="o">=</span><span class="n">dfhoggs</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

</pre></div>
</div>
</div>
</div>
<div class="section" id="Sample">
<h3>Sample<a class="headerlink" href="#Sample" title="Permalink to this headline">¶</a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">mdl_studentt</span><span class="p">:</span>
    <span class="c1">## take samples</span>
    <span class="n">traces_studentt</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">tune</span><span class="o">=</span><span class="mi">2000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (2 chains in 2 jobs)
NUTS: [nu, b1_slope, b0_intercept]
Sampling 2 chains: 100%|██████████| 5000/5000 [00:03&lt;00:00, 1324.87draws/s]
The acceptance probability does not match the target. It is 0.6456398252635402, but should be close to 0.8. Try to increase the number of tuning steps.
</pre></div></div>
</div>
<div class="section" id="View-Traces">
<h4>View Traces<a class="headerlink" href="#View-Traces" title="Permalink to this headline">¶</a></h4>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">_</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">traceplot</span><span class="p">(</span><span class="n">traces_studentt</span><span class="p">,</span>
                 <span class="n">lines</span><span class="o">=</span><span class="nb">tuple</span><span class="p">([(</span><span class="n">k</span><span class="p">,</span> <span class="p">{},</span> <span class="n">v</span><span class="p">[</span><span class="s1">&#39;mean&#39;</span><span class="p">])</span>
                              <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">pm</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">traces_studentt</span><span class="p">)</span><span class="o">.</span><span class="n">iterrows</span><span class="p">()]))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_GLM-robust-with-outlier-detection_24_0.png" src="../_images/notebooks_GLM-robust-with-outlier-detection_24_0.png" />
</div>
</div>
<p><strong>Observe:</strong></p>
<ul class="simple">
<li><p>Both parameters <code class="docutils literal notranslate"><span class="pre">b0</span></code> and <code class="docutils literal notranslate"><span class="pre">b1</span></code> show quite a skew to the right, possibly this is the action of a few samples regressing closer to the OLS estimate which is towards the left</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">nu</span></code> parameter seems very happy to stick at <code class="docutils literal notranslate"><span class="pre">nu</span> <span class="pre">=</span> <span class="pre">1</span></code>, indicating that a fat-tailed Student-T likelihood has a better fit than a thin-tailed (Normal-like) Student-T likelihood.</p></li>
<li><p>The inference sampling also ran very quickly, almost as quickly as the conventional OLS</p></li>
</ul>
<p><strong>NOTE:</strong> We’ll illustrate this Student-T fit and compare to the datapoints in the final plot</p>
</div>
</div>
</div>
<div class="section" id="Create-Robust-Model-with-Outliers:-Hogg-Method">
<h2>Create Robust Model with Outliers: Hogg Method<a class="headerlink" href="#Create-Robust-Model-with-Outliers:-Hogg-Method" title="Permalink to this headline">¶</a></h2>
<p>Please read the paper (Hogg 2010) and Jake Vanderplas’ code for more complete information about the modelling technique.</p>
<p>The general idea is to create a ‘mixture’ model whereby datapoints can be described by either the linear model (inliers) or a modified linear model with different mean and larger variance (outliers).</p>
<p>The likelihood is evaluated over a mixture of two likelihoods, one for ‘inliers’, one for ‘outliers’. A Bernouilli distribution is used to randomly assign datapoints in N to either the inlier or outlier groups, and we sample the model as usual to infer robust model parameters and inlier / outlier flags:</p>
<div class="math notranslate nohighlight">
\[\mathcal{logL} = \sum_{i}^{i=N} log \left[ \frac{(1 - B_{i})}{\sqrt{2 \pi \sigma_{in}^{2}}} exp \left( - \frac{(x_{i} - \mu_{in})^{2}}{2\sigma_{in}^{2}} \right) \right] + \sum_{i}^{i=N} log \left[ \frac{B_{i}}{\sqrt{2 \pi (\sigma_{in}^{2} + \sigma_{out}^{2})}} exp \left( - \frac{(x_{i}- \mu_{out})^{2}}{2(\sigma_{in}^{2} + \sigma_{out}^{2})} \right) \right]\]</div>
<div class="line-block">
<div class="line">where:</div>
<div class="line"><span class="math notranslate nohighlight">\(\bf{B}\)</span> is Bernoulli-distibuted <span class="math notranslate nohighlight">\(B_{i} \in [0_{(inlier)},1_{(outlier)}]\)</span></div>
</div>
<p>Note: A previous version of this example implemented the above likelihood directly in theano. However, we can implement it more efficiently using the Normal logp from PyMC3 and a <code class="docutils literal notranslate"><span class="pre">Potential</span></code>.</p>
<div class="section" id="Define-model">
<h3>Define model<a class="headerlink" href="#Define-model" title="Permalink to this headline">¶</a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">mdl_signoise</span><span class="p">:</span>

    <span class="c1">## Define informative Normal priors to give Ridge regression</span>
    <span class="n">b0</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;b0_intercept&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">testval</span><span class="o">=</span><span class="n">pm</span><span class="o">.</span><span class="n">floatX</span><span class="p">(</span><span class="mf">0.1</span><span class="p">))</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;b1_slope&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">testval</span><span class="o">=</span><span class="n">pm</span><span class="o">.</span><span class="n">floatX</span><span class="p">(</span><span class="mf">1.</span><span class="p">))</span>

    <span class="c1">## Define linear model</span>
    <span class="n">yest_in</span> <span class="o">=</span> <span class="n">b0</span> <span class="o">+</span> <span class="n">b1</span> <span class="o">*</span> <span class="n">dfhoggs</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span>

    <span class="c1">## Define weakly informative priors for the mean and variance of outliers</span>
    <span class="n">yest_out</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;yest_out&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">testval</span><span class="o">=</span><span class="n">pm</span><span class="o">.</span><span class="n">floatX</span><span class="p">(</span><span class="mf">1.</span><span class="p">))</span>
    <span class="n">sigma_y_out</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s1">&#39;sigma_y_out&#39;</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">testval</span><span class="o">=</span><span class="n">pm</span><span class="o">.</span><span class="n">floatX</span><span class="p">(</span><span class="mf">1.</span><span class="p">))</span>

    <span class="c1">## Define Bernoulli inlier / outlier flags according to a hyperprior</span>
    <span class="c1">## fraction of outliers, itself constrained to [0, .5] for symmetry</span>
    <span class="n">frac_outliers</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s1">&#39;frac_outliers&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">upper</span><span class="o">=.</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">is_outlier</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="s1">&#39;is_outlier&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">frac_outliers</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">dfhoggs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                              <span class="n">testval</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">dfhoggs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">&lt;</span> <span class="mf">0.2</span><span class="p">)</span>

    <span class="c1">## Extract observed y and sigma_y from dataset, encode as theano objects</span>
    <span class="n">yobs</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">dfhoggs</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span><span class="p">))</span>
    <span class="n">sigma_y_in</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">dfhoggs</span><span class="p">[</span><span class="s1">&#39;sigma_y&#39;</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span><span class="p">)</span>

    <span class="c1"># Set up normal distributions that give us the logp for both distributions</span>
    <span class="n">inliers</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="o">.</span><span class="n">dist</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">yest_in</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma_y_in</span><span class="p">)</span><span class="o">.</span><span class="n">logp</span><span class="p">(</span><span class="n">yobs</span><span class="p">)</span>
    <span class="n">outliers</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="o">.</span><span class="n">dist</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">yest_out</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma_y_in</span> <span class="o">+</span> <span class="n">sigma_y_out</span><span class="p">)</span><span class="o">.</span><span class="n">logp</span><span class="p">(</span><span class="n">yobs</span><span class="p">)</span>
    <span class="c1"># Build custom likelihood, a potential will just be added to the logp and can thus function</span>
    <span class="c1"># like a likelihood that we would add with the observed kwarg.</span>
    <span class="n">pm</span><span class="o">.</span><span class="n">Potential</span><span class="p">(</span><span class="s1">&#39;obs&#39;</span><span class="p">,</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">is_outlier</span><span class="p">)</span> <span class="o">*</span> <span class="n">inliers</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">+</span> <span class="p">(</span><span class="n">is_outlier</span> <span class="o">*</span> <span class="n">outliers</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Sample">
<h3>Sample<a class="headerlink" href="#Sample" title="Permalink to this headline">¶</a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">mdl_signoise</span><span class="p">:</span>
    <span class="n">traces_signoise</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">tune</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Multiprocess sampling (2 chains in 2 jobs)
CompoundStep
&gt;NUTS: [frac_outliers, sigma_y_out, yest_out, b1_slope, b0_intercept]
&gt;BinaryGibbsMetropolis: [is_outlier]
Sampling 2 chains: 100%|██████████| 11000/11000 [00:20&lt;00:00, 529.35draws/s]
There were 11 divergences after tuning. Increase `target_accept` or reparameterize.
The acceptance probability does not match the target. It is 0.89434265257995, but should be close to 0.8. Try to increase the number of tuning steps.
The number of effective samples is smaller than 25% for some parameters.
</pre></div></div>
</div>
<p>There are some divergences, and because we explicitly modeling the latent label (outliner or not) the sampler could have problem. Rewritting this model into a marginal mixture model would be better.</p>
</div>
<div class="section" id="View-Traces">
<h3>View Traces<a class="headerlink" href="#View-Traces" title="Permalink to this headline">¶</a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">varnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;b0_intercept&#39;</span><span class="p">,</span> <span class="s1">&#39;b1_slope&#39;</span><span class="p">,</span> <span class="s1">&#39;yest_out&#39;</span><span class="p">,</span> <span class="s1">&#39;sigma_y_out&#39;</span><span class="p">,</span> <span class="s1">&#39;frac_outliers&#39;</span><span class="p">]</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">traceplot</span><span class="p">(</span><span class="n">traces_signoise</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="n">varnames</span><span class="p">,</span>
                 <span class="n">lines</span><span class="o">=</span><span class="nb">tuple</span><span class="p">([(</span><span class="n">k</span><span class="p">,</span> <span class="p">{},</span> <span class="n">v</span><span class="p">[</span><span class="s1">&#39;mean&#39;</span><span class="p">])</span>
                              <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">pm</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">traces_signoise</span><span class="p">,</span> <span class="n">varnames</span><span class="o">=</span><span class="n">varnames</span><span class="p">)</span><span class="o">.</span><span class="n">iterrows</span><span class="p">()]))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_GLM-robust-with-outlier-detection_34_0.png" src="../_images/notebooks_GLM-robust-with-outlier-detection_34_0.png" />
</div>
</div>
<p><strong>NOTE:</strong></p>
<ul class="simple">
<li><p>During development I’ve found that 3 datapoints id=[1,2,3] are always indicated as outliers, but the remaining ordering of datapoints by decreasing outlier-hood is unstable between runs: the posterior surface appears to have a small number of solutions with very similar probability.</p></li>
</ul>
</div>
</div>
<div class="section" id="Declare-Outliers-and-Compare-Plots">
<h2>Declare Outliers and Compare Plots<a class="headerlink" href="#Declare-Outliers-and-Compare-Plots" title="Permalink to this headline">¶</a></h2>
<div class="section" id="View-ranges-for-inliers-/-outlier-predictions">
<h3>View ranges for inliers / outlier predictions<a class="headerlink" href="#View-ranges-for-inliers-/-outlier-predictions" title="Permalink to this headline">¶</a></h3>
<p>At each step of the traces, each datapoint may be either an inlier or outlier. We hope that the datapoints spend an unequal time being one state or the other, so let’s take a look at the simple count of states for each of the 20 datapoints.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">outlier_melt</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">melt</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">traces_signoise</span><span class="p">[</span><span class="s1">&#39;is_outlier&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1000</span><span class="p">:],</span>
                                   <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;[</span><span class="si">{}</span><span class="s1">]&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">d</span><span class="p">))</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">dfhoggs</span><span class="o">.</span><span class="n">index</span><span class="p">]),</span>
                      <span class="n">var_name</span><span class="o">=</span><span class="s1">&#39;datapoint_id&#39;</span><span class="p">,</span> <span class="n">value_name</span><span class="o">=</span><span class="s1">&#39;is_outlier&#39;</span><span class="p">)</span>
<span class="n">ax0</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">pointplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="s1">&#39;datapoint_id&#39;</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;is_outlier&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">outlier_melt</span><span class="p">,</span>
                   <span class="n">kind</span><span class="o">=</span><span class="s1">&#39;point&#39;</span><span class="p">,</span> <span class="n">join</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ci</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">ax0</span><span class="o">.</span><span class="n">vlines</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">,</span><span class="s1">&#39;r&#39;</span><span class="p">],</span> <span class="s1">&#39;--&#39;</span><span class="p">)</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">ax0</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">((</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">1.1</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax0</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax0</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s1">&#39;</span><span class="si">{:.0%}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">1.1</span><span class="p">,</span><span class="mf">0.1</span><span class="p">)])</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">ax0</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">&#39;major&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax0</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Prop. of the trace where datapoint is an outlier&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax0</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Prop. of the trace where is_outlier == 1&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_GLM-robust-with-outlier-detection_39_0.png" src="../_images/notebooks_GLM-robust-with-outlier-detection_39_0.png" />
</div>
</div>
<p><strong>Observe</strong>:</p>
<ul class="simple">
<li><p>The plot above shows the number of samples in the traces in which each datapoint is marked as an outlier, expressed as a percentage.</p></li>
<li><p>In particular, 3 points [1, 2, 3] spend &gt;=95% of their time as outliers</p></li>
<li><p>Contrastingly, points at the other end of the plot close to 0% are our strongest inliers.</p></li>
<li><p>For comparison, the mean posterior value of <code class="docutils literal notranslate"><span class="pre">frac_outliers</span></code> is ~0.35, corresponding to roughly 7 of the 20 datapoints. You can see these 7 datapoints in the plot above, all those with a value &gt;50% or thereabouts.</p></li>
<li><p>However, only 3 of these points are outliers &gt;=95% of the time.</p></li>
<li><p>See note above regarding instability between runs.</p></li>
</ul>
<p>The 95% cutoff we choose is subjective and arbitrary, but I prefer it for now, so let’s declare these 3 to be outliers and see how it looks compared to Jake Vanderplas’ outliers, which were declared in a slightly different way as points with means above 0.68.</p>
</div>
<div class="section" id="Declare-outliers">
<h3>Declare outliers<a class="headerlink" href="#Declare-outliers" title="Permalink to this headline">¶</a></h3>
<p><strong>Note:</strong> + I will declare outliers to be datapoints that have value == 1 at the 5-percentile cutoff, i.e. in the percentiles from 5 up to 100, their values are 1. + Try for yourself altering cutoff to larger values, which leads to an objective ranking of outlier-hood.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">cutoff</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">dfhoggs</span><span class="p">[</span><span class="s1">&#39;outlier&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">traces_signoise</span><span class="p">[</span><span class="s1">&#39;is_outlier&#39;</span><span class="p">],</span> <span class="n">cutoff</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">dfhoggs</span><span class="p">[</span><span class="s1">&#39;outlier&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
0.0    17
1.0     3
Name: outlier, dtype: int64
</pre></div></div>
</div>
</div>
<div class="section" id="Posterior-Prediction-Plots-for-OLS-vs-StudentT-vs-SignalNoise">
<h3>Posterior Prediction Plots for OLS vs StudentT vs SignalNoise<a class="headerlink" href="#Posterior-Prediction-Plots-for-OLS-vs-StudentT-vs-SignalNoise" title="Permalink to this headline">¶</a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">matplotlib.lines</span> <span class="kn">import</span> <span class="n">Line2D</span>

<span class="n">g</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">FacetGrid</span><span class="p">(</span><span class="n">dfhoggs</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s1">&#39;outlier&#39;</span><span class="p">,</span> <span class="n">hue_order</span><span class="o">=</span><span class="p">[</span><span class="kc">True</span><span class="p">,</span><span class="kc">False</span><span class="p">],</span>
                  <span class="n">palette</span><span class="o">=</span><span class="s1">&#39;Set1&#39;</span><span class="p">,</span> <span class="n">legend_out</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">lm</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">samp</span><span class="p">:</span> <span class="n">samp</span><span class="p">[</span><span class="s1">&#39;b0_intercept&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">samp</span><span class="p">[</span><span class="s1">&#39;b1_slope&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span>

<span class="n">pm</span><span class="o">.</span><span class="n">plot_posterior_predictive_glm</span><span class="p">(</span><span class="n">traces_ols</span><span class="p">,</span>
        <span class="nb">eval</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">lm</span><span class="o">=</span><span class="n">lm</span><span class="p">,</span> <span class="n">samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#22CC00&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">2</span><span class="p">)</span>

<span class="n">pm</span><span class="o">.</span><span class="n">plot_posterior_predictive_glm</span><span class="p">(</span><span class="n">traces_studentt</span><span class="p">,</span> <span class="n">lm</span><span class="o">=</span><span class="n">lm</span><span class="p">,</span>
        <span class="nb">eval</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#FFA500&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">5</span><span class="p">)</span>

<span class="n">pm</span><span class="o">.</span><span class="n">plot_posterior_predictive_glm</span><span class="p">(</span><span class="n">traces_signoise</span><span class="p">,</span> <span class="n">lm</span><span class="o">=</span><span class="n">lm</span><span class="p">,</span>
        <span class="nb">eval</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#357EC7&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">3</span><span class="p">)</span>

<span class="n">ols_line</span> <span class="o">=</span> <span class="n">Line2D</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#22CC00&#39;</span><span class="p">)</span>
<span class="n">studentt_line</span> <span class="o">=</span> <span class="n">Line2D</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#FFA500&#39;</span><span class="p">)</span>
<span class="n">hogg_line</span> <span class="o">=</span> <span class="n">Line2D</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#357EC7&#39;</span><span class="p">)</span>
<span class="n">line_legend</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="n">ols_line</span><span class="p">,</span> <span class="n">studentt_line</span><span class="p">,</span> <span class="n">hogg_line</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;OLS&#39;</span><span class="p">,</span> <span class="s1">&#39;Student-T&#39;</span><span class="p">,</span> <span class="s1">&#39;Hogg&#39;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">add_artist</span><span class="p">(</span><span class="n">line_legend</span><span class="p">)</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">errorbar</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;sigma_y&#39;</span><span class="p">,</span> <span class="s1">&#39;sigma_x&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">add_legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">ylims</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">xlims</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_GLM-robust-with-outlier-detection_44_0.png" src="../_images/notebooks_GLM-robust-with-outlier-detection_44_0.png" />
</div>
</div>
<p><strong>Observe</strong>:</p>
<ul class="simple">
<li><p>The posterior preditive fit for:</p>
<ul>
<li><p>the <strong>OLS model</strong> is shown in <strong>Green</strong> and as expected, it doesn’t appear to fit the majority of our datapoints very well, skewed by outliers</p></li>
<li><p>the <strong>Robust Student-T model</strong> is shown in <strong>Orange</strong> and does appear to fit the ‘main axis’ of datapoints quite well, ignoring outliers</p></li>
<li><p>the <strong>Robust Signal vs Noise model</strong> is shown in <strong>Blue</strong> and also appears to fit the ‘main axis’ of datapoints rather well, ignoring outliers.</p></li>
</ul>
</li>
<li><p>We see that the <strong>Robust Signal vs Noise model</strong> also yields specific estimates of <em>which</em> datapoints are outliers:</p>
<ul>
<li><p>17 ‘inlier’ datapoints, in <strong>Blue</strong> and</p></li>
<li><p>3 ‘outlier’ datapoints shown in <strong>Red</strong>.</p></li>
<li><p>From a simple visual inspection, the classification seems fair, and agrees with Jake Vanderplas’ findings.</p></li>
</ul>
</li>
<li><p>Overall, it seems that:</p>
<ul>
<li><p>the <strong>Signal vs Noise model</strong> behaves as promised, yielding a robust regression estimate and explicit labelling of inliers / outliers, but</p></li>
<li><p>the <strong>Signal vs Noise model</strong> is quite complex and whilst the regression seems robust and stable, the actual inlier / outlier labelling seems slightly unstable</p></li>
<li><p>if you simply want a robust regression without inlier / outlier labelling, the <strong>Student-T model</strong> may be a good compromise, offering a simple model, quick sampling, and a very similar estimate.</p></li>
</ul>
</li>
</ul>
<p>Example originally contributed by Jonathan Sedar 2015-12-21 <a class="reference external" href="https://github.com/jonsedar">github.com/jonsedar</a>. Updated by Thomas Wiecki 2018-7-24.</p>
</div>
</div>
</div>


    </div>
</div>
<div class="ui vertical footer segment">
    <div class="ui center aligned container">
        <a href="https://github.com/pymc-devs/pymc3"><i class="github icon large"></i></a>
        <a href="https://twitter.com/pymc_devs"><i class="twitter icon large"></i></a>
        <a href="https://discourse.pymc.io/"><i class="discourse icon large"></i></a>
    </div>
    <div class="ui center aligned container">
        <p>
            &copy; Copyright 2018, The PyMC Development Team.
        </p>
        <p>
            Created using <a href="https://sphinx-doc.org/">Sphinx</a> 2.2.1.<br />
        </p>
    </div>
</div>
  </body>
</html>