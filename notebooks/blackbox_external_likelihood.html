
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Using a “black box” likelihood function &#8212; PyMC3 3.6 documentation</title>
    <link rel="stylesheet" href="../_static/semantic-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/semantic-ui@2.4.2/dist/semantic.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/default.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/language_data.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"processClass": "math|output_area", "processEscapes": true, "ignoreClass": "document", "inlineMath": [["$", "$"], ["\\(", "\\)"]]}})</script>
    <script type="text/javascript" src="../_static/highlight.min.js"></script>
    <script type="text/javascript" src="../_static/semantic.min.js"></script>
    <link rel="shortcut icon" href="../_static/PyMC3.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
<script>hljs.initHighlightingOnLoad();</script>
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">



  </head><body>
<div class="ui vertical center aligned">

    <div class="ui container">
        <div class="ui large secondary pointing menu">
            <a class="item" href="/">
                <img class="ui bottom aligned tiny image" src="https://cdn.rawgit.com/pymc-devs/pymc3/master/docs/logos/svg/PyMC3_banner.svg" />
            </a>
             <a href="../nb_tutorials/index.html" class="item">Tutorials</a> <a href="../nb_examples/index.html" class="item">Examples</a> <a href="../learn.html" class="item">Books + Videos</a> <a href="../api.html" class="item">API</a> <a href="../developer_guide.html" class="item">Developer Guide</a> <a href="../history.html" class="item">About PyMC3</a>
            
            <div class="right menu">
                <div class="item">
                    <form class="ui icon input" action="../search.html" method="get">
                        <input type="text" placeholder="Search..." name="q" />
                        <i class="search link icon"></i>
                    </form>
                </div>
                <a class="item" href="https://github.com/pymc-devs/pymc3"><i class="github blue icon large"></i></a>
            </div>
        </div>
    </div>
    
</div>

<div class="ui container" role="main">
    

    <div class="ui vertical segment">
        
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 7ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    background: #f5f5f5;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
</style>
<div class="section" id="Using-a-“black-box”-likelihood-function">
<h1>Using a “black box” likelihood function<a class="headerlink" href="#Using-a-“black-box”-likelihood-function" title="Permalink to this headline">¶</a></h1>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="o">%</span><span class="k">load_ext</span> Cython

<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="kn">import</span> <span class="nn">pymc3</span> <span class="kn">as</span> <span class="nn">pm</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;seaborn-darkgrid&#39;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Running on PyMC3 v{}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pm</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>

<span class="c1"># for reproducibility here&#39;s some version info for modules used in this notebook</span>
<span class="kn">import</span> <span class="nn">theano</span>
<span class="kn">import</span> <span class="nn">theano.tensor</span> <span class="kn">as</span> <span class="nn">tt</span>
<span class="kn">import</span> <span class="nn">platform</span>
<span class="kn">import</span> <span class="nn">cython</span>
<span class="kn">import</span> <span class="nn">IPython</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">emcee</span>
<span class="kn">import</span> <span class="nn">corner</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Python version:     {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">platform</span><span class="o">.</span><span class="n">python_version</span><span class="p">()))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;IPython version:    {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">IPython</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Cython version:     {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cython</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;GSL version:        {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">popen</span><span class="p">(</span><span class="s1">&#39;gsl-config --version&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">()))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Numpy version:      {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Theano version:     {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">theano</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Matplotlib version: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">matplotlib</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;emcee version:      {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">emcee</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;corner version:     {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">corner</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Running on PyMC3 v3.5
Python version:     3.6.6
IPython version:    6.5.0
Cython version:     0.28.5
GSL version:        2.1
Numpy version:      1.15.1
Theano version:     1.0.2
Matplotlib version: 2.2.3
emcee version:      2.2.1
corner version:     2.0.1
</pre></div></div>
</div>
<p><a class="reference external" href="https://docs.pymc.io/index.html">PyMC3</a> is a great tool for doing Bayesian inference and parameter estimation. It has a load of <a class="reference external" href="https://docs.pymc.io/api/distributions.html">in-built probability distributions</a> that you can use to set up priors and likelihood functions for your particular model. You can even create your own <a class="reference external" href="https://docs.pymc.io/prob_dists.html#custom-distributions">custom distributions</a>.</p>
<p>However, this is not necessarily that simple if you have a model function, or probability distribution, that, for example, relies on an external code that you have little/no control over (and may even be, for example, wrapped <code class="docutils literal notranslate"><span class="pre">C</span></code> code rather than Python). This can be problematic went you need to pass parameters set as PyMC3 distributions to these external functions; your external function probably wants you to pass it floating point numbers rather than PyMC3 distributions!</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pymc3</span> <span class="kn">as</span> <span class="nn">pm</span><span class="p">:</span>
<span class="kn">from</span> <span class="nn">external_module</span> <span class="kn">import</span> <span class="n">my_external_func</span>  <span class="c1"># your external function!</span>

<span class="c1"># set up your model</span>
<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">():</span>
    <span class="c1"># your external function takes two parameters, a and b, with Uniform priors</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>

    <span class="n">m</span> <span class="o">=</span> <span class="n">my_external_func</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>  <span class="c1"># &lt;--- this is not going to work!</span>
</pre></div>
</div>
<p>Another issue is that if you want to be able to use the <a class="reference external" href="https://docs.pymc.io/notebooks/getting_started.html#Gradient-based-sampling-methods">gradient-based step samplers</a> like <a class="reference external" href="https://docs.pymc.io/api/inference.html#module-pymc3.step_methods.hmc.nuts">NUTS</a> and <a class="reference external" href="https://docs.pymc.io/api/inference.html#hamiltonian-monte-carlo">Hamiltonian Monte Carlo (HMC)</a>, then your model/likelihood needs a gradient to be defined. If you have a model that is defined as a set of Theano operators then
this is no problem - internally it will be able to do automatic differentiation - but if your model is essentially a “black box” then you won’t necessarily know what the gradients are.</p>
<p>Defining a model/likelihood that PyMC3 can use and that calls your “black box” function is possible, but it relies on creating a <a class="reference external" href="https://docs.pymc.io/advanced_theano.html#writing-custom-theano-ops">custom Theano Op</a>. This is, hopefully, a clear description of how to do this, including one way of writing a gradient function that could be generally applicable.</p>
<p>In the examples below, we create a very simple model and log-likelihood function in <a class="reference external" href="http://cython.org/">Cython</a>. Cython is used just as an example to show what you might need to do if calling external <code class="docutils literal notranslate"><span class="pre">C</span></code> codes, but you could in fact be using pure Python codes. The log-likelihood function used is actually just a <a class="reference external" href="https://en.wikipedia.org/wiki/Normal_distribution">Normal distribution</a>, so defining this yourself is obviously overkill (and I’ll compare it to doing the same thing purely
with the pre-defined PyMC3 <a class="reference external" href="https://docs.pymc.io/api/distributions/continuous.html#pymc3.distributions.continuous.Normal">Normal</a> distribution), but it should provide a simple to follow demonstration.</p>
<p>First, let’s define a <em>super-complicated</em>™ model (a straight line!), which is parameterised by two variables (a gradient <code class="docutils literal notranslate"><span class="pre">m</span></code> and a y-intercept <code class="docutils literal notranslate"><span class="pre">c</span></code>) and calculated at a vector of points <code class="docutils literal notranslate"><span class="pre">x</span></code>. Here the model is defined in <a class="reference external" href="http://cython.org/">Cython</a> and calls <a class="reference external" href="https://www.gnu.org/software/gsl/">GSL</a> functions. This is just to show that you could be calling some other <code class="docutils literal notranslate"><span class="pre">C</span></code> library that you need. In this example, the model parameters are all packed into a list/array/tuple called
<code class="docutils literal notranslate"><span class="pre">theta</span></code>.</p>
<p>Let’s also define a <em>really-complicated</em>™ log-likelihood function (a Normal log-likelihood that ignores the normalisation), which takes in the list/array/tuple of model parameter values <code class="docutils literal notranslate"><span class="pre">theta</span></code>, the points at which to calculate the model <code class="docutils literal notranslate"><span class="pre">x</span></code>, the vector of “observed” data points <code class="docutils literal notranslate"><span class="pre">data</span></code>, and the standard deviation of the noise in the data <code class="docutils literal notranslate"><span class="pre">sigma</span></code>. This log-likelihood function calls the <em>super-complicated</em>™ model function.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-cython notranslate"><div class="highlight"><pre>
<span></span><span class="o">%%</span><span class="n">cython</span> <span class="o">-</span><span class="n">I</span><span class="o">/</span><span class="n">usr</span><span class="o">/</span><span class="k">include</span> <span class="o">-</span><span class="n">L</span><span class="o">/</span><span class="n">usr</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">gnu</span> <span class="o">-</span><span class="n">lgsl</span> <span class="o">-</span><span class="n">lgslcblas</span> <span class="o">-</span><span class="n">lm</span>

<span class="k">import</span> <span class="nn">cython</span>
<span class="k">cimport</span> <span class="nn">cython</span>

<span class="k">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="k">cimport</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c">### STUFF FOR USING GSL (FEEL FREE TO IGNORE!) ###</span>

<span class="c"># declare GSL vector structure and functions</span>
<span class="k">cdef</span> <span class="kr">extern</span> <span class="k">from</span> <span class="s">&quot;gsl/gsl_block.h&quot;</span><span class="p">:</span>
    <span class="k">cdef</span> <span class="k">struct</span> <span class="nf">gsl_block</span><span class="p">:</span>
        <span class="n">size_t</span> <span class="n">size</span>
        <span class="n">double</span> <span class="o">*</span> <span class="n">data</span>

<span class="k">cdef</span> <span class="kr">extern</span> <span class="k">from</span> <span class="s">&quot;gsl/gsl_vector.h&quot;</span><span class="p">:</span>
    <span class="k">cdef</span> <span class="k">struct</span> <span class="nf">gsl_vector</span><span class="p">:</span>
        <span class="n">size_t</span> <span class="n">size</span>
        <span class="n">size_t</span> <span class="n">stride</span>
        <span class="n">double</span> <span class="o">*</span> <span class="n">data</span>
        <span class="n">gsl_block</span> <span class="o">*</span> <span class="n">block</span>
        <span class="nb">int</span> <span class="n">owner</span>

    <span class="k">ctypedef</span> <span class="k">struct</span> <span class="nc">gsl_vector_view</span><span class="p">:</span>
        <span class="n">gsl_vector</span> <span class="n">vector</span>

    <span class="nb">int</span> <span class="n">gsl_vector_scale</span> <span class="p">(</span><span class="n">gsl_vector</span> <span class="o">*</span> <span class="n">a</span><span class="p">,</span> <span class="n">const</span> <span class="n">double</span> <span class="n">x</span><span class="p">)</span> <span class="k">nogil</span>
    <span class="nb">int</span> <span class="n">gsl_vector_add_constant</span> <span class="p">(</span><span class="n">gsl_vector</span> <span class="o">*</span> <span class="n">a</span><span class="p">,</span> <span class="n">const</span> <span class="n">double</span> <span class="n">x</span><span class="p">)</span> <span class="k">nogil</span>
    <span class="n">gsl_vector_view</span> <span class="n">gsl_vector_view_array</span> <span class="p">(</span><span class="n">double</span> <span class="o">*</span> <span class="n">base</span><span class="p">,</span> <span class="n">size_t</span> <span class="n">n</span><span class="p">)</span> <span class="k">nogil</span>

<span class="c">###################################################</span>


<span class="c"># define your super-complicated model that uses loads of external codes</span>
<span class="k">cpdef</span> <span class="nf">my_model</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float64_t</span><span class="p">,</span> <span class="n">ndim</span><span class="o">=</span><span class="mf">1</span><span class="p">]</span> <span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A straight line!</span>

<span class="sd">    Note:</span>
<span class="sd">        This function could simply be:</span>

<span class="sd">            m, c = thetha</span>
<span class="sd">            return m*x + x</span>

<span class="sd">        but I&#39;ve made it more complicated for demonstration purposes</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">theta</span>  <span class="c"># unpack line gradient and y-intercept</span>

    <span class="k">cdef</span> <span class="kt">size_t</span> <span class="nf">length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c"># length of x</span>

    <span class="k">cdef</span> <span class="kt">np</span>.<span class="kt">ndarray</span> <span class="nf">line</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c"># make copy of x vector</span>
    <span class="k">cdef</span> <span class="kt">gsl_vector_view</span> <span class="nf">lineview</span>      <span class="c"># create a view of the vector</span>
    <span class="n">lineview</span> <span class="o">=</span> <span class="n">gsl_vector_view_array</span><span class="p">(</span><span class="o">&lt;</span><span class="n">double</span> <span class="o">*&gt;</span><span class="n">line</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">length</span><span class="p">)</span>

    <span class="c"># multiply x by m</span>
    <span class="n">gsl_vector_scale</span><span class="p">(</span><span class="o">&amp;</span><span class="n">lineview</span><span class="o">.</span><span class="n">vector</span><span class="p">,</span> <span class="p">&lt;</span><span class="kt">double</span><span class="p">&gt;</span><span class="n">m</span><span class="p">)</span>

    <span class="c"># add c</span>
    <span class="n">gsl_vector_add_constant</span><span class="p">(</span><span class="o">&amp;</span><span class="n">lineview</span><span class="o">.</span><span class="n">vector</span><span class="p">,</span> <span class="p">&lt;</span><span class="kt">double</span><span class="p">&gt;</span><span class="n">c</span><span class="p">)</span>

    <span class="c"># return the numpy array</span>
    <span class="k">return</span> <span class="n">line</span>


<span class="c"># define your really-complicated likelihood function that uses loads of external codes</span>
<span class="k">cpdef</span> <span class="nf">my_loglike</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float64_t</span><span class="p">,</span> <span class="n">ndim</span><span class="o">=</span><span class="mf">1</span><span class="p">]</span> <span class="n">x</span><span class="p">,</span>
                 <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float64_t</span><span class="p">,</span> <span class="n">ndim</span><span class="o">=</span><span class="mf">1</span><span class="p">]</span> <span class="n">data</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A Gaussian log-likelihood function for a model with parameters given in theta</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">my_model</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

    <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="mf">0.5</span><span class="o">/</span><span class="n">sigma</span><span class="o">**</span><span class="mf">2</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">data</span> <span class="o">-</span> <span class="n">model</span><span class="p">)</span><span class="o">**</span><span class="mf">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Now, as things are, if we wanted to sample from this log-likelihood function, using certain prior distributions for the model parameters (gradient and y-intercept) using PyMC3, we might try something like this (using a <a class="reference external" href="https://docs.pymc.io/prob_dists.html#custom-distributions">PyMC3 DensityDist</a>):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pymc3</span> <span class="kn">as</span> <span class="nn">pm</span>

<span class="c1"># create/read in our &quot;data&quot; (I&#39;ll show this in the real example below)</span>
<span class="n">x</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">data</span> <span class="o">=</span> <span class="o">...</span>

<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">():</span>
    <span class="c1"># set priors on model gradient and y-intercept</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=-</span><span class="mf">10.</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">10.</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=-</span><span class="mf">10.</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">10.</span><span class="p">)</span>

    <span class="c1"># create custom distribution</span>
    <span class="n">pm</span><span class="o">.</span><span class="n">DensityDist</span><span class="p">(</span><span class="s1">&#39;likelihood&#39;</span><span class="p">,</span> <span class="n">my_loglike</span><span class="p">,</span>
                   <span class="n">observed</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;theta&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">c</span><span class="p">),</span> <span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">:</span> <span class="n">data</span><span class="p">,</span> <span class="s1">&#39;sigma&#39;</span><span class="p">:</span> <span class="n">sigma</span><span class="p">})</span>

    <span class="c1"># sample from the distribution</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
<p>But, this will give an error like:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ne">ValueError</span><span class="p">:</span> <span class="n">setting</span> <span class="n">an</span> <span class="n">array</span> <span class="n">element</span> <span class="k">with</span> <span class="n">a</span> <span class="n">sequence</span><span class="o">.</span>
</pre></div>
</div>
<p>This is because <code class="docutils literal notranslate"><span class="pre">m</span></code> and <code class="docutils literal notranslate"><span class="pre">c</span></code> are Theano tensor-type objects.</p>
<p>So, what we actually need to do is create a <a class="reference external" href="http://deeplearning.net/software/theano/extending/extending_theano.html">Theano Op</a>. This will be a new class that wraps our log-likelihood function (or just our model function, if that is all that is required) into something that can take in Theano tensor objects, but internally can cast them as floating point values that can be passed to our log-likelihood function. We will do this below, initially without defining a <a class="reference external" href="http://deeplearning.net/software/theano/extending/op.html#grad">grad()
method</a> for the Op.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># define a theano Op for our likelihood function</span>
<span class="k">class</span> <span class="nc">LogLike</span><span class="p">(</span><span class="n">tt</span><span class="o">.</span><span class="n">Op</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Specify what type of object will be passed and returned to the Op when it is</span>
<span class="sd">    called. In our case we will be passing it a vector of values (the parameters</span>
<span class="sd">    that define our model) and returning a single &quot;scalar&quot; value (the</span>
<span class="sd">    log-likelihood)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">itypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">tt</span><span class="o">.</span><span class="n">dvector</span><span class="p">]</span> <span class="c1"># expects a vector of parameter values when called</span>
    <span class="n">otypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">tt</span><span class="o">.</span><span class="n">dscalar</span><span class="p">]</span> <span class="c1"># outputs a single scalar value (the log likelihood)</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loglike</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialise the Op with various things that our log-likelihood function</span>
<span class="sd">        requires. Below are the things that are needed in this particular</span>
<span class="sd">        example.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        loglike:</span>
<span class="sd">            The log-likelihood (or whatever) function we&#39;ve defined</span>
<span class="sd">        data:</span>
<span class="sd">            The &quot;observed&quot; data that our log-likelihood function takes in</span>
<span class="sd">        x:</span>
<span class="sd">            The dependent variable (aka &#39;x&#39;) that our model requires</span>
<span class="sd">        sigma:</span>
<span class="sd">            The noise standard deviation that our function requires.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># add inputs as class attributes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span> <span class="o">=</span> <span class="n">loglike</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>

    <span class="k">def</span> <span class="nf">perform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
        <span class="c1"># the method that is used when calling the Op</span>
        <span class="n">theta</span><span class="p">,</span> <span class="o">=</span> <span class="n">inputs</span>  <span class="c1"># this will contain my variables</span>

        <span class="c1"># call the log-likelihood function</span>
        <span class="n">logl</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span><span class="p">)</span>

        <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">logl</span><span class="p">)</span> <span class="c1"># output the log-likelihood</span>
</pre></div>
</div>
</div>
<p>Now, let’s use this Op to repeat the example shown above. To do this let’s create some data containing a straight line with additive Gaussian noise (with a mean of zero and a standard deviation of <code class="docutils literal notranslate"><span class="pre">sigma</span></code>). For simplicity we set <a class="reference external" href="https://docs.pymc.io/api/distributions/continuous.html#pymc3.distributions.continuous.Uniform">uniform</a> prior distributions on the gradient and y-intercept. As we’ve not set the <code class="docutils literal notranslate"><span class="pre">grad()</span></code> method of the Op PyMC3 will not be able to use the gradient-based samplers,
so will fall back to using the <a class="reference external" href="https://docs.pymc.io/api/inference.html#module-pymc3.step_methods.slicer">Slice</a> sampler.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># set up our data</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># number of data points</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="mf">1.</span>  <span class="c1"># standard deviation of noise</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>

<span class="n">mtrue</span> <span class="o">=</span> <span class="mf">0.4</span>  <span class="c1"># true gradient</span>
<span class="n">ctrue</span> <span class="o">=</span> <span class="mf">3.</span>   <span class="c1"># true y-intercept</span>

<span class="n">truemodel</span> <span class="o">=</span> <span class="n">my_model</span><span class="p">([</span><span class="n">mtrue</span><span class="p">,</span> <span class="n">ctrue</span><span class="p">],</span> <span class="n">x</span><span class="p">)</span>

<span class="c1"># make data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">716742</span><span class="p">)</span>  <span class="c1"># set random seed, so the data is reproducible each time</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">sigma</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span> <span class="o">+</span> <span class="n">truemodel</span>

<span class="n">ndraws</span> <span class="o">=</span> <span class="mi">3000</span>  <span class="c1"># number of draws from the distribution</span>
<span class="n">nburn</span> <span class="o">=</span> <span class="mi">1000</span>   <span class="c1"># number of &quot;burn-in points&quot; (which we&#39;ll discard)</span>

<span class="c1"># create our Op</span>
<span class="n">logl</span> <span class="o">=</span> <span class="n">LogLike</span><span class="p">(</span><span class="n">my_loglike</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>

<span class="c1"># use PyMC3 to sampler from log-likelihood</span>
<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">():</span>
    <span class="c1"># uniform priors on m and c</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=-</span><span class="mf">10.</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">10.</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=-</span><span class="mf">10.</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">10.</span><span class="p">)</span>

    <span class="c1"># convert m and c to a tensor vector</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">tt</span><span class="o">.</span><span class="n">as_tensor_variable</span><span class="p">([</span><span class="n">m</span><span class="p">,</span> <span class="n">c</span><span class="p">])</span>

    <span class="c1"># use a DensityDist (use a lamdba function to &quot;call&quot; the Op)</span>
    <span class="n">pm</span><span class="o">.</span><span class="n">DensityDist</span><span class="p">(</span><span class="s1">&#39;likelihood&#39;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">logl</span><span class="p">(</span><span class="n">v</span><span class="p">),</span> <span class="n">observed</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;v&#39;</span><span class="p">:</span> <span class="n">theta</span><span class="p">})</span>

    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">ndraws</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="n">nburn</span><span class="p">,</span> <span class="n">discard_tuned_samples</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># plot the traces</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">traceplot</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">lines</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;m&#39;</span><span class="p">:</span> <span class="n">mtrue</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">:</span> <span class="n">ctrue</span><span class="p">})</span>

<span class="c1"># put the chains in an array (for later!)</span>
<span class="n">samples_pymc3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">trace</span><span class="p">[</span><span class="s1">&#39;m&#39;</span><span class="p">],</span> <span class="n">trace</span><span class="p">[</span><span class="s1">&#39;c&#39;</span><span class="p">]))</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Initializing NUTS failed. Falling back to elementwise auto-assignment.
Multiprocess sampling (2 chains in 2 jobs)
CompoundStep
&gt;Slice: [c]
&gt;Slice: [m]
Sampling 2 chains: 100%|██████████| 8000/8000 [00:04&lt;00:00, 1928.75draws/s]
The number of effective samples is smaller than 25% for some parameters.
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_blackbox_external_likelihood_7_1.png" src="../_images/notebooks_blackbox_external_likelihood_7_1.png" />
</div>
</div>
<p>What if we wanted to use NUTS or HMC? If we knew the analytical derivatives of the model/likelihood function then we could add a <a class="reference external" href="http://deeplearning.net/software/theano/extending/op.html#grad">grad() method</a> to the Op using that analytical form.</p>
<p>But, what if we don’t know the analytical form. If our model/likelihood is purely Python and made up of standard maths operators and Numpy functions, then the <a class="reference external" href="https://github.com/HIPS/autograd">autograd</a> module could potentially be used to find gradients (also, see <a class="reference external" href="https://github.com/ActiveState/code/blob/master/recipes/Python/580610_Auto_differentiation/recipe-580610.py">here</a> for a nice Python example of automatic differentiation). But, if our model/likelihood truely is a “black box”
then we can just use the good-old-fashioned <a class="reference external" href="https://en.wikipedia.org/wiki/Finite_difference">finite difference</a> to find the gradients - this can be slow, especially if there are a large number of variables, or the model takes a long time to evaluate. Below, a function to find gradients has been defined that uses the finite difference (the central difference) - it uses an iterative method with successively smaller interval sizes to check that the gradient converges. But, you could do
something far simpler and just use, for example, the SciPy <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.approx_fprime.html">approx_fprime</a> function. Here, the gradient function is defined in Cython for speed, but if the function it evaluates to find the gradients is the performance bottle neck then having this as a pure Python function may not make a significant speed difference.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-cython notranslate"><div class="highlight"><pre>
<span></span><span class="o">%%</span><span class="n">cython</span>

<span class="k">import</span> <span class="nn">cython</span>
<span class="k">cimport</span> <span class="nn">cython</span>

<span class="k">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="k">cimport</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">import</span> <span class="nn">warnings</span>

<span class="k">def</span> <span class="nf">gradients</span><span class="p">(</span><span class="n">vals</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">releps</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">abseps</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">mineps</span><span class="o">=</span><span class="mf">1e-9</span><span class="p">,</span> <span class="n">reltol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
              <span class="n">epsscale</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the partial derivatives of a function at a set of values. The</span>
<span class="sd">    derivatives are calculated using the central difference, using an iterative</span>
<span class="sd">    method to check that the values converge as step size decreases.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    vals: array_like</span>
<span class="sd">        A set of values, that are passed to a function, at which to calculate</span>
<span class="sd">        the gradient of that function</span>
<span class="sd">    func:</span>
<span class="sd">        A function that takes in an array of values.</span>
<span class="sd">    releps: float, array_like, 1e-3</span>
<span class="sd">        The initial relative step size for calculating the derivative.</span>
<span class="sd">    abseps: float, array_like, None</span>
<span class="sd">        The initial absolute step size for calculating the derivative.</span>
<span class="sd">        This overrides `releps` if set.</span>
<span class="sd">        `releps` is set then that is used.</span>
<span class="sd">    mineps: float, 1e-9</span>
<span class="sd">        The minimum relative step size at which to stop iterations if no</span>
<span class="sd">        convergence is achieved.</span>
<span class="sd">    epsscale: float, 0.5</span>
<span class="sd">        The factor by which releps if scaled in each iteration.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    grads: array_like</span>
<span class="sd">        An array of gradients for each non-fixed value.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">grads</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vals</span><span class="p">))</span>

    <span class="c"># maximum number of times the gradient can change sign</span>
    <span class="n">flipflopmax</span> <span class="o">=</span> <span class="mf">10.</span>

    <span class="c"># set steps</span>
    <span class="k">if</span> <span class="n">abseps</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">releps</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">vals</span><span class="p">)</span><span class="o">*</span><span class="n">releps</span>
            <span class="n">eps</span><span class="p">[</span><span class="n">eps</span> <span class="o">==</span> <span class="mf">0.</span><span class="p">]</span> <span class="o">=</span> <span class="n">releps</span>  <span class="c"># if any values are zero set eps to releps</span>
            <span class="n">teps</span> <span class="o">=</span> <span class="n">releps</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vals</span><span class="p">))</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">releps</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">releps</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vals</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&quot;Problem with input relative step sizes&quot;</span><span class="p">)</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">vals</span><span class="p">),</span> <span class="n">releps</span><span class="p">)</span>
            <span class="n">eps</span><span class="p">[</span><span class="n">eps</span> <span class="o">==</span> <span class="mf">0.</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">releps</span><span class="p">)[</span><span class="n">eps</span> <span class="o">==</span> <span class="mf">0.</span><span class="p">]</span>
            <span class="n">teps</span> <span class="o">=</span> <span class="n">releps</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s">&quot;Relative step sizes are not a recognised type!&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">abseps</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="n">abseps</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vals</span><span class="p">))</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">abseps</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">abseps</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vals</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&quot;Problem with input absolute step sizes&quot;</span><span class="p">)</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">abseps</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s">&quot;Absolute step sizes are not a recognised type!&quot;</span><span class="p">)</span>
        <span class="n">teps</span> <span class="o">=</span> <span class="n">eps</span>

    <span class="c"># for each value in vals calculate the gradient</span>
    <span class="n">count</span> <span class="o">=</span> <span class="mf">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vals</span><span class="p">)):</span>
        <span class="c"># initial parameter diffs</span>
        <span class="n">leps</span> <span class="o">=</span> <span class="n">eps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">cureps</span> <span class="o">=</span> <span class="n">teps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

        <span class="n">flipflop</span> <span class="o">=</span> <span class="mf">0</span>

        <span class="c"># get central finite difference</span>
        <span class="n">fvals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">vals</span><span class="p">)</span>
        <span class="n">bvals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">vals</span><span class="p">)</span>

        <span class="c"># central difference</span>
        <span class="n">fvals</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">leps</span>  <span class="c"># change forwards distance to half eps</span>
        <span class="n">bvals</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">leps</span>  <span class="c"># change backwards distance to half eps</span>
        <span class="n">cdiff</span> <span class="o">=</span> <span class="p">(</span><span class="n">func</span><span class="p">(</span><span class="n">fvals</span><span class="p">)</span><span class="o">-</span><span class="n">func</span><span class="p">(</span><span class="n">bvals</span><span class="p">))</span><span class="o">/</span><span class="n">leps</span>

        <span class="k">while</span> <span class="mf">1</span><span class="p">:</span>
            <span class="n">fvals</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">leps</span>  <span class="c"># remove old step</span>
            <span class="n">bvals</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">leps</span>

            <span class="c"># change the difference by a factor of two</span>
            <span class="n">cureps</span> <span class="o">*=</span> <span class="n">epsscale</span>
            <span class="k">if</span> <span class="n">cureps</span> <span class="o">&lt;</span> <span class="n">mineps</span> <span class="ow">or</span> <span class="n">flipflop</span> <span class="o">&gt;</span> <span class="n">flipflopmax</span><span class="p">:</span>
                <span class="c"># if no convergence set flat derivative (TODO: check if there is a better thing to do instead)</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s">&quot;Derivative calculation did not converge: setting flat derivative.&quot;</span><span class="p">)</span>
                <span class="n">grads</span><span class="p">[</span><span class="n">count</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.</span>
                <span class="k">break</span>
            <span class="n">leps</span> <span class="o">*=</span> <span class="n">epsscale</span>

            <span class="c"># central difference</span>
            <span class="n">fvals</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">leps</span>  <span class="c"># change forwards distance to half eps</span>
            <span class="n">bvals</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">leps</span>  <span class="c"># change backwards distance to half eps</span>
            <span class="n">cdiffnew</span> <span class="o">=</span> <span class="p">(</span><span class="n">func</span><span class="p">(</span><span class="n">fvals</span><span class="p">)</span><span class="o">-</span><span class="n">func</span><span class="p">(</span><span class="n">bvals</span><span class="p">))</span><span class="o">/</span><span class="n">leps</span>

            <span class="k">if</span> <span class="n">cdiffnew</span> <span class="o">==</span> <span class="n">cdiff</span><span class="p">:</span>
                <span class="n">grads</span><span class="p">[</span><span class="n">count</span><span class="p">]</span> <span class="o">=</span> <span class="n">cdiff</span>
                <span class="k">break</span>

            <span class="c"># check whether previous diff and current diff are the same within reltol</span>
            <span class="n">rat</span> <span class="o">=</span> <span class="p">(</span><span class="n">cdiff</span><span class="o">/</span><span class="n">cdiffnew</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">rat</span><span class="p">)</span> <span class="ow">and</span> <span class="n">rat</span> <span class="o">&gt;</span> <span class="mf">0.</span><span class="p">:</span>
                <span class="c"># gradient has not changed sign</span>
                <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="mf">1.</span><span class="o">-</span><span class="n">rat</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">reltol</span><span class="p">:</span>
                    <span class="n">grads</span><span class="p">[</span><span class="n">count</span><span class="p">]</span> <span class="o">=</span> <span class="n">cdiffnew</span>
                    <span class="k">break</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">cdiff</span> <span class="o">=</span> <span class="n">cdiffnew</span>
                    <span class="k">continue</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">cdiff</span> <span class="o">=</span> <span class="n">cdiffnew</span>
                <span class="n">flipflop</span> <span class="o">+=</span> <span class="mf">1</span>
                <span class="k">continue</span>

        <span class="n">count</span> <span class="o">+=</span> <span class="mf">1</span>

    <span class="k">return</span> <span class="n">grads</span>
</pre></div>
</div>
</div>
<p>So, now we can just redefine our Op with a <code class="docutils literal notranslate"><span class="pre">grad()</span></code> method, right?</p>
<p>It’s not quite so simple! The <code class="docutils literal notranslate"><span class="pre">grad()</span></code> method itself requires that its inputs are Theano tensor variables, whereas our <code class="docutils literal notranslate"><span class="pre">gradients</span></code> function above, like our <code class="docutils literal notranslate"><span class="pre">my_loglike</span></code> function, wants a list of floating point values. So, we need to define another Op that calculates the gradients. Below, I define a new version of the <code class="docutils literal notranslate"><span class="pre">LogLike</span></code> Op, called <code class="docutils literal notranslate"><span class="pre">LogLikeWithGrad</span></code> this time, that has a <code class="docutils literal notranslate"><span class="pre">grad()</span></code> method. This is followed by anothor Op called <code class="docutils literal notranslate"><span class="pre">LogLikeGrad</span></code> that, when called with a vector of
Theano tensor variables, returns another vector of values that are the gradients (i.e., the <a class="reference external" href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant">Jacobian</a>) of our log-likelihood function at those values. Note that the <code class="docutils literal notranslate"><span class="pre">grad()</span></code> method itself does not return the gradients directly, but instead returns the <a class="reference external" href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant">Jacobian</a>-vector product (you can hopefully just copy what I’ve done and not worry about what this means too
much!).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># define a theano Op for our likelihood function</span>
<span class="k">class</span> <span class="nc">LogLikeWithGrad</span><span class="p">(</span><span class="n">tt</span><span class="o">.</span><span class="n">Op</span><span class="p">):</span>

    <span class="n">itypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">tt</span><span class="o">.</span><span class="n">dvector</span><span class="p">]</span> <span class="c1"># expects a vector of parameter values when called</span>
    <span class="n">otypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">tt</span><span class="o">.</span><span class="n">dscalar</span><span class="p">]</span> <span class="c1"># outputs a single scalar value (the log likelihood)</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loglike</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialise with various things that the function requires. Below</span>
<span class="sd">        are the things that are needed in this particular example.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        loglike:</span>
<span class="sd">            The log-likelihood (or whatever) function we&#39;ve defined</span>
<span class="sd">        data:</span>
<span class="sd">            The &quot;observed&quot; data that our log-likelihood function takes in</span>
<span class="sd">        x:</span>
<span class="sd">            The dependent variable (aka &#39;x&#39;) that our model requires</span>
<span class="sd">        sigma:</span>
<span class="sd">            The noise standard deviation that out function requires.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># add inputs as class attributes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span> <span class="o">=</span> <span class="n">loglike</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>

        <span class="c1"># initialise the gradient Op (below)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logpgrad</span> <span class="o">=</span> <span class="n">LogLikeGrad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">perform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
        <span class="c1"># the method that is used when calling the Op</span>
        <span class="n">theta</span><span class="p">,</span> <span class="o">=</span> <span class="n">inputs</span>  <span class="c1"># this will contain my variables</span>

        <span class="c1"># call the log-likelihood function</span>
        <span class="n">logl</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span><span class="p">)</span>

        <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">logl</span><span class="p">)</span> <span class="c1"># output the log-likelihood</span>

    <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">g</span><span class="p">):</span>
        <span class="c1"># the method that calculates the gradients - it actually returns the</span>
        <span class="c1"># vector-Jacobian product - g[0] is a vector of parameter values</span>
        <span class="n">theta</span><span class="p">,</span> <span class="o">=</span> <span class="n">inputs</span>  <span class="c1"># our parameters</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">g</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">logpgrad</span><span class="p">(</span><span class="n">theta</span><span class="p">)]</span>


<span class="k">class</span> <span class="nc">LogLikeGrad</span><span class="p">(</span><span class="n">tt</span><span class="o">.</span><span class="n">Op</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This Op will be called with a vector of values and also return a vector of</span>
<span class="sd">    values - the gradients in each dimension.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">itypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">tt</span><span class="o">.</span><span class="n">dvector</span><span class="p">]</span>
    <span class="n">otypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">tt</span><span class="o">.</span><span class="n">dvector</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loglike</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialise with various things that the function requires. Below</span>
<span class="sd">        are the things that are needed in this particular example.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        loglike:</span>
<span class="sd">            The log-likelihood (or whatever) function we&#39;ve defined</span>
<span class="sd">        data:</span>
<span class="sd">            The &quot;observed&quot; data that our log-likelihood function takes in</span>
<span class="sd">        x:</span>
<span class="sd">            The dependent variable (aka &#39;x&#39;) that our model requires</span>
<span class="sd">        sigma:</span>
<span class="sd">            The noise standard deviation that out function requires.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># add inputs as class attributes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span> <span class="o">=</span> <span class="n">loglike</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>

    <span class="k">def</span> <span class="nf">perform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
        <span class="n">theta</span><span class="p">,</span> <span class="o">=</span> <span class="n">inputs</span>

        <span class="c1"># define version of likelihood function to pass to derivative function</span>
        <span class="k">def</span> <span class="nf">lnlike</span><span class="p">(</span><span class="n">values</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">likelihood</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span><span class="p">)</span>

        <span class="c1"># calculate gradients</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="n">gradients</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">lnlike</span><span class="p">)</span>

        <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">grads</span>
</pre></div>
</div>
</div>
<p>Now, let’s re-run PyMC3 with our new “grad”-ed Op. This time it will be able to automatically use NUTS.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># create our Op</span>
<span class="n">logl</span> <span class="o">=</span> <span class="n">LogLikeWithGrad</span><span class="p">(</span><span class="n">my_loglike</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>

<span class="c1"># use PyMC3 to sampler from log-likelihood</span>
<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">opmodel</span><span class="p">:</span>
    <span class="c1"># uniform priors on m and c</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=-</span><span class="mf">10.</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">10.</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=-</span><span class="mf">10.</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">10.</span><span class="p">)</span>

    <span class="c1"># convert m and c to a tensor vector</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">tt</span><span class="o">.</span><span class="n">as_tensor_variable</span><span class="p">([</span><span class="n">m</span><span class="p">,</span> <span class="n">c</span><span class="p">])</span>

    <span class="c1"># use a DensityDist</span>
    <span class="n">pm</span><span class="o">.</span><span class="n">DensityDist</span><span class="p">(</span><span class="s1">&#39;likelihood&#39;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">logl</span><span class="p">(</span><span class="n">v</span><span class="p">),</span> <span class="n">observed</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;v&#39;</span><span class="p">:</span> <span class="n">theta</span><span class="p">})</span>

    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">ndraws</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="n">nburn</span><span class="p">,</span> <span class="n">discard_tuned_samples</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># plot the traces</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">traceplot</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">lines</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;m&#39;</span><span class="p">:</span> <span class="n">mtrue</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">:</span> <span class="n">ctrue</span><span class="p">})</span>

<span class="c1"># put the chains in an array (for later!)</span>
<span class="n">samples_pymc3_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">trace</span><span class="p">[</span><span class="s1">&#39;m&#39;</span><span class="p">],</span> <span class="n">trace</span><span class="p">[</span><span class="s1">&#39;c&#39;</span><span class="p">]))</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (2 chains in 2 jobs)
NUTS: [c, m]
Sampling 2 chains: 100%|██████████| 8000/8000 [00:08&lt;00:00, 898.88draws/s]
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_blackbox_external_likelihood_13_1.png" src="../_images/notebooks_blackbox_external_likelihood_13_1.png" />
</div>
</div>
<p>Now, finally, just to check things actually worked as we might expect, let’s do the same thing purely using PyMC3 distributions (because in this simple example we can!)</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">pymodel</span><span class="p">:</span>
    <span class="c1"># uniform priors on m and c</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=-</span><span class="mf">10.</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">10.</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=-</span><span class="mf">10.</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">10.</span><span class="p">)</span>

    <span class="c1"># convert m and c to a tensor vector</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">tt</span><span class="o">.</span><span class="n">as_tensor_variable</span><span class="p">([</span><span class="n">m</span><span class="p">,</span> <span class="n">c</span><span class="p">])</span>

    <span class="c1"># use a Normal distribution</span>
    <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;likelihood&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="p">(</span><span class="n">m</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">c</span><span class="p">),</span> <span class="n">sd</span> <span class="o">=</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>

    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">ndraws</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="n">nburn</span><span class="p">,</span> <span class="n">discard_tuned_samples</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># plot the traces</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">traceplot</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">lines</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;m&#39;</span><span class="p">:</span> <span class="n">mtrue</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">:</span> <span class="n">ctrue</span><span class="p">})</span>

<span class="c1"># put the chains in an array (for later!)</span>
<span class="n">samples_pymc3_3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">trace</span><span class="p">[</span><span class="s1">&#39;m&#39;</span><span class="p">],</span> <span class="n">trace</span><span class="p">[</span><span class="s1">&#39;c&#39;</span><span class="p">]))</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (2 chains in 2 jobs)
NUTS: [c, m]
Sampling 2 chains: 100%|██████████| 8000/8000 [00:03&lt;00:00, 2049.77draws/s]
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_blackbox_external_likelihood_15_1.png" src="../_images/notebooks_blackbox_external_likelihood_15_1.png" />
</div>
</div>
<p>To check that they match let’s plot all the examples together and also find the autocorrelation lengths.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">FutureWarning</span><span class="p">)</span>  <span class="c1"># supress emcee autocorr FutureWarning</span>

<span class="n">matplotlib</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;font.size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">18</span>

<span class="n">hist2dkwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;plot_datapoints&#39;</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
                <span class="s1">&#39;plot_density&#39;</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
                <span class="s1">&#39;levels&#39;</span><span class="p">:</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)}</span> <span class="c1"># roughly 1 and 2 sigma</span>

<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Theanp Op (no grad)&#39;</span><span class="p">,</span> <span class="s1">&#39;Theano Op (with grad)&#39;</span><span class="p">,</span> <span class="s1">&#39;Pure PyMC3&#39;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">samples</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="n">samples_pymc3</span><span class="p">,</span> <span class="n">samples_pymc3_2</span><span class="p">,</span> <span class="n">samples_pymc3_3</span><span class="p">]):</span>
    <span class="c1"># get maximum chain autocorrelartion length</span>
    <span class="n">autocorrlen</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">emcee</span><span class="o">.</span><span class="n">autocorr</span><span class="o">.</span><span class="n">integrated_time</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="mi">3</span><span class="p">)));</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Auto-correlation length ({}): {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">autocorrlen</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">fig</span> <span class="o">=</span> <span class="n">corner</span><span class="o">.</span><span class="n">corner</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="sa">r</span><span class="s2">&quot;$m$&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;$c$&quot;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                            <span class="n">hist_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;density&#39;</span><span class="p">:</span> <span class="bp">True</span><span class="p">},</span> <span class="o">**</span><span class="n">hist2dkwargs</span><span class="p">,</span>
                            <span class="n">truths</span><span class="o">=</span><span class="p">[</span><span class="n">mtrue</span><span class="p">,</span> <span class="n">ctrue</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">corner</span><span class="o">.</span><span class="n">corner</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">hist_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;density&#39;</span><span class="p">:</span> <span class="bp">True</span><span class="p">},</span>
                      <span class="n">fig</span><span class="o">=</span><span class="n">fig</span><span class="p">,</span> <span class="o">**</span><span class="n">hist2dkwargs</span><span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Auto-correlation length (Theanp Op (no grad)): 6
Auto-correlation length (Theano Op (with grad)): 3
Auto-correlation length (Pure PyMC3): 3
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_blackbox_external_likelihood_17_1.png" src="../_images/notebooks_blackbox_external_likelihood_17_1.png" />
</div>
</div>
<p>We can now check that the gradient Op works was we expect it to. First, just create and call the <code class="docutils literal notranslate"><span class="pre">LogLikeGrad</span></code> class, which should return the gradient directly (note that we have to create a <a class="reference external" href="http://deeplearning.net/software/theano/library/compile/function.html">Theano function</a> to convert the output of the Op to an array). Secondly, we call the gradient from <code class="docutils literal notranslate"><span class="pre">LogLikeWithGrad</span></code> by using the <a class="reference external" href="http://deeplearning.net/software/theano/library/gradient.html#theano.gradient.grad">Theano tensor
gradient</a> function. Finally, we will check the gradient returned by the PyMC3 model for a Normal distribution, which should be the same as the log-likelihood function we defined. In all cases we evaluate the gradients at the true values of the model function (the straight line) that was created.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># test the gradient Op by direct call</span>
<span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">compute_test_value</span> <span class="o">=</span> <span class="s2">&quot;ignore&quot;</span>
<span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">exception_verbosity</span> <span class="o">=</span> <span class="s2">&quot;high&quot;</span>

<span class="n">var</span> <span class="o">=</span> <span class="n">tt</span><span class="o">.</span><span class="n">dvector</span><span class="p">()</span>
<span class="n">test_grad_op</span> <span class="o">=</span> <span class="n">LogLikeGrad</span><span class="p">(</span><span class="n">my_loglike</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
<span class="n">test_grad_op_func</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">([</span><span class="n">var</span><span class="p">],</span> <span class="n">test_grad_op</span><span class="p">(</span><span class="n">var</span><span class="p">))</span>
<span class="n">grad_vals</span> <span class="o">=</span> <span class="n">test_grad_op_func</span><span class="p">([</span><span class="n">mtrue</span><span class="p">,</span> <span class="n">ctrue</span><span class="p">])</span>

<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Gradient returned by &quot;LogLikeGrad&quot;: {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">grad_vals</span><span class="p">))</span>

<span class="c1"># test the gradient called through LogLikeWithGrad</span>
<span class="n">test_gradded_op</span> <span class="o">=</span> <span class="n">LogLikeWithGrad</span><span class="p">(</span><span class="n">my_loglike</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
<span class="n">test_gradded_op_grad</span> <span class="o">=</span> <span class="n">tt</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">test_gradded_op</span><span class="p">(</span><span class="n">var</span><span class="p">),</span> <span class="n">var</span><span class="p">)</span>
<span class="n">test_gradded_op_grad_func</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">([</span><span class="n">var</span><span class="p">],</span> <span class="n">test_gradded_op_grad</span><span class="p">)</span>
<span class="n">grad_vals_2</span> <span class="o">=</span> <span class="n">test_gradded_op_grad_func</span><span class="p">([</span><span class="n">mtrue</span><span class="p">,</span> <span class="n">ctrue</span><span class="p">])</span>

<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Gradient returned by &quot;LogLikeWithGrad&quot;: {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">grad_vals_2</span><span class="p">))</span>

<span class="c1"># test the gradient that PyMC3 uses for the Normal log likelihood</span>
<span class="n">test_model</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span>
<span class="k">with</span> <span class="n">test_model</span><span class="p">:</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=-</span><span class="mf">10.</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">10.</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=-</span><span class="mf">10.</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">10.</span><span class="p">)</span>

    <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;likelihood&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="p">(</span><span class="n">m</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">c</span><span class="p">),</span> <span class="n">sd</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>

    <span class="n">gradfunc</span> <span class="o">=</span> <span class="n">test_model</span><span class="o">.</span><span class="n">logp_dlogp_function</span><span class="p">([</span><span class="n">m</span><span class="p">,</span> <span class="n">c</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
    <span class="n">gradfunc</span><span class="o">.</span><span class="n">set_extra_values</span><span class="p">({</span><span class="s1">&#39;m_interval__&#39;</span><span class="p">:</span> <span class="n">mtrue</span><span class="p">,</span> <span class="s1">&#39;c_interval__&#39;</span><span class="p">:</span> <span class="n">ctrue</span><span class="p">})</span>
    <span class="n">grad_vals_pymc3</span> <span class="o">=</span> <span class="n">gradfunc</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">mtrue</span><span class="p">,</span> <span class="n">ctrue</span><span class="p">]))[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># get dlogp values</span>

<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Gradient returned by PyMC3 &quot;Normal&quot; distribution: {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">grad_vals_pymc3</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Gradient returned by &#34;LogLikeGrad&#34;: [-7.17656625 -1.39486358]
Gradient returned by &#34;LogLikeWithGrad&#34;: [-7.17656625 -1.39486358]
Gradient returned by PyMC3 &#34;Normal&#34; distribution: [-7.17656625 -1.39486358]
</pre></div></div>
</div>
<p>We can also do some <a class="reference external" href="http://docs.pymc.io/notebooks/profiling.html">profiling</a> of the Op, as used within a PyMC3 Model, to check performance. First, we’ll profile using the <code class="docutils literal notranslate"><span class="pre">LogLikeWithGrad</span></code> Op, and then doing the same thing purely using PyMC3 distributions.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># profile logpt using our Op</span>
<span class="n">opmodel</span><span class="o">.</span><span class="n">profile</span><span class="p">(</span><span class="n">opmodel</span><span class="o">.</span><span class="n">logpt</span><span class="p">)</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Function profiling
==================
  Message: /home/matthew/.conda/envs/testing/lib/python3.6/site-packages/pymc3/model.py:909
  Time in 1000 calls to Function.__call__: 4.123712e-02s
  Time in Function.fn.__call__: 2.523708e-02s (61.200%)
  Time in thunks: 2.199244e-02s (53.332%)
  Total compile time: 7.463312e-02s
    Number of Apply nodes: 8
    Theano Optimizer time: 5.898285e-02s
       Theano validate time: 5.080700e-04s
    Theano Linker time (includes C, CUDA code generation/compiling): 4.571915e-03s
       Import time 0.000000e+00s
       Node make_thunk time 4.115105e-03s
           Node Elemwise{Composite{((i0 + Switch(Cast{int8}((GE(i1, i2) * LE(i1, i3))), i4, i5)) - ((i6 * scalar_softplus((-i7))) + i7))}}[(0, 1)](TensorConstant{2.995732273553991}, c, TensorConstant{-10.0}, TensorConstant{10.0}, TensorConstant{-2.995732273553991}, TensorConstant{-inf}, TensorConstant{2.0}, c_interval__) time 8.506775e-04s
           Node Elemwise{Composite{((i0 + Switch(Cast{int8}((GE(i1, i2) * LE(i1, i3))), i4, i5)) - ((i6 * scalar_softplus((-i7))) + i7))}}[(0, 1)](TensorConstant{2.995732273553991}, m, TensorConstant{-10.0}, TensorConstant{10.0}, TensorConstant{-2.995732273553991}, TensorConstant{-inf}, TensorConstant{2.0}, m_interval__) time 8.454323e-04s
           Node Elemwise{Composite{(i0 + (i1 * scalar_sigmoid(i2)))}}(TensorConstant{-10.0}, TensorConstant{20.0}, c_interval__) time 6.031990e-04s
           Node Elemwise{Composite{(i0 + (i1 * scalar_sigmoid(i2)))}}(TensorConstant{-10.0}, TensorConstant{20.0}, m_interval__) time 5.083084e-04s
           Node &lt;__main__.LogLikeWithGrad object at 0x7f1d606e8fd0&gt;(MakeVector{dtype=&#39;float64&#39;}.0) time 3.864765e-04s

Time in all call to theano.grad() 6.874464e-01s
Time since theano import 43.624s
Class
---
&lt;% time&gt; &lt;sum %&gt; &lt;apply time&gt; &lt;time per call&gt; &lt;type&gt; &lt;#call&gt; &lt;#apply&gt; &lt;Class name&gt;
  91.8%    91.8%       0.020s       2.02e-05s     Py    1000       1   __main__.LogLikeWithGrad
   4.5%    96.3%       0.001s       2.48e-07s     C     4000       4   theano.tensor.elemwise.Elemwise
   2.8%    99.1%       0.001s       3.04e-07s     C     2000       2   theano.tensor.opt.MakeVector
   0.9%   100.0%       0.000s       2.06e-07s     C     1000       1   theano.tensor.elemwise.Sum
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
&lt;% time&gt; &lt;sum %&gt; &lt;apply time&gt; &lt;time per call&gt; &lt;type&gt; &lt;#call&gt; &lt;#apply&gt; &lt;Op name&gt;
  91.8%    91.8%       0.020s       2.02e-05s     Py    1000        1   &lt;__main__.LogLikeWithGrad object at 0x7f1d606e8fd0&gt;
   2.8%    94.6%       0.001s       3.04e-07s     C     2000        2   MakeVector{dtype=&#39;float64&#39;}
   2.5%    97.1%       0.001s       2.79e-07s     C     2000        2   Elemwise{Composite{(i0 + (i1 * scalar_sigmoid(i2)))}}
   2.0%    99.1%       0.000s       2.16e-07s     C     2000        2   Elemwise{Composite{((i0 + Switch(Cast{int8}((GE(i1, i2) * LE(i1, i3))), i4, i5)) - ((i6 * scalar_softplus((-i7))) + i7))}}[(0, 1)]
   0.9%   100.0%       0.000s       2.06e-07s     C     1000        1   Sum{acc_dtype=float64}
   ... (remaining 0 Ops account for   0.00%(0.00s) of the runtime)

Apply
------
&lt;% time&gt; &lt;sum %&gt; &lt;apply time&gt; &lt;time per call&gt; &lt;#call&gt; &lt;id&gt; &lt;Apply name&gt;
  91.8%    91.8%       0.020s       2.02e-05s   1000     5   &lt;__main__.LogLikeWithGrad object at 0x7f1d606e8fd0&gt;(MakeVector{dtype=&#39;float64&#39;}.0)
   2.0%    93.8%       0.000s       4.42e-07s   1000     1   Elemwise{Composite{(i0 + (i1 * scalar_sigmoid(i2)))}}(TensorConstant{-10.0}, TensorConstant{20.0}, m_interval__)
   1.7%    95.5%       0.000s       3.73e-07s   1000     6   MakeVector{dtype=&#39;float64&#39;}(__logp_m_interval__, __logp_c_interval__, __logp_likelihood)
   1.5%    97.0%       0.000s       3.30e-07s   1000     4   Elemwise{Composite{((i0 + Switch(Cast{int8}((GE(i1, i2) * LE(i1, i3))), i4, i5)) - ((i6 * scalar_softplus((-i7))) + i7))}}[(0, 1)](TensorConstant{2.995732273553991}, m, TensorConstant{-10.0}, TensorConstant{10.0}, TensorConstant{-2.995732273553991}, TensorConstant{-inf}, TensorConstant{2.0}, m_interval__)
   1.1%    98.1%       0.000s       2.35e-07s   1000     2   MakeVector{dtype=&#39;float64&#39;}(m, c)
   0.9%    99.0%       0.000s       2.06e-07s   1000     7   Sum{acc_dtype=float64}(MakeVector{dtype=&#39;float64&#39;}.0)
   0.5%    99.5%       0.000s       1.16e-07s   1000     0   Elemwise{Composite{(i0 + (i1 * scalar_sigmoid(i2)))}}(TensorConstant{-10.0}, TensorConstant{20.0}, c_interval__)
   0.5%   100.0%       0.000s       1.03e-07s   1000     3   Elemwise{Composite{((i0 + Switch(Cast{int8}((GE(i1, i2) * LE(i1, i3))), i4, i5)) - ((i6 * scalar_softplus((-i7))) + i7))}}[(0, 1)](TensorConstant{2.995732273553991}, c, TensorConstant{-10.0}, TensorConstant{10.0}, TensorConstant{-2.995732273553991}, TensorConstant{-inf}, TensorConstant{2.0}, c_interval__)
   ... (remaining 0 Apply instances account for 0.00%(0.00s) of the runtime)

Here are tips to potentially make your code run faster
                 (if you think of new ones, suggest them on the mailing list).
                 Test them first, as they are not guaranteed to always provide a speedup.
  - Try the Theano flag floatX=float32
  - Try installing amdlibm and set the Theano flag lib.amdlibm=True. This speeds up only some Elemwise operation.
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># profile using our PyMC3 distribution</span>
<span class="n">pymodel</span><span class="o">.</span><span class="n">profile</span><span class="p">(</span><span class="n">pymodel</span><span class="o">.</span><span class="n">logpt</span><span class="p">)</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Function profiling
==================
  Message: /home/matthew/.conda/envs/testing/lib/python3.6/site-packages/pymc3/model.py:909
  Time in 1000 calls to Function.__call__: 1.982212e-02s
  Time in Function.fn.__call__: 5.885363e-03s (29.691%)
  Time in thunks: 2.546072e-03s (12.845%)
  Total compile time: 1.308653e-01s
    Number of Apply nodes: 11
    Theano Optimizer time: 1.083095e-01s
       Theano validate time: 8.940697e-04s
    Theano Linker time (includes C, CUDA code generation/compiling): 8.928537e-03s
       Import time 1.179457e-03s
       Node make_thunk time 8.301497e-03s
           Node Elemwise{Composite{(i0 * scalar_sigmoid(i1))}}(TensorConstant{20.0}, c_interval__) time 1.806974e-03s
           Node Elemwise{Composite{((i0 + Switch(Cast{int8}((GE((i1 + i2), i1) * LE((i1 + i2), i3))), i4, i5)) - ((i6 * scalar_softplus((-i7))) + i7))}}[(0, 2)](TensorConstant{2.995732273553991}, TensorConstant{-10.0}, Elemwise{Composite{(i0 * scalar_sigmoid(i1))}}.0, TensorConstant{10.0}, TensorConstant{-2.995732273553991}, TensorConstant{-inf}, TensorConstant{2.0}, c_interval__) time 1.499176e-03s
           Node InplaceDimShuffle{x}(Elemwise{Composite{(i0 * scalar_sigmoid(i1))}}.0) time 8.523464e-04s
           Node Elemwise{Composite{((i0 + Switch(Cast{int8}((GE(i1, i2) * LE(i1, i3))), i4, i5)) - ((i6 * scalar_softplus((-i7))) + i7))}}[(0, 1)](TensorConstant{2.995732273553991}, m, TensorConstant{-10.0}, TensorConstant{10.0}, TensorConstant{-2.995732273553991}, TensorConstant{-inf}, TensorConstant{2.0}, m_interval__) time 7.934570e-04s
           Node InplaceDimShuffle{x}(m) time 7.789135e-04s

Time in all call to theano.grad() 6.874464e-01s
Time since theano import 43.954s
Class
---
&lt;% time&gt; &lt;sum %&gt; &lt;apply time&gt; &lt;time per call&gt; &lt;type&gt; &lt;#call&gt; &lt;#apply&gt; &lt;Class name&gt;
  53.3%    53.3%       0.001s       2.26e-07s     C     6000       6   theano.tensor.elemwise.Elemwise
  22.9%    76.2%       0.001s       2.91e-07s     C     2000       2   theano.tensor.elemwise.DimShuffle
  14.1%    90.3%       0.000s       3.58e-07s     C     1000       1   theano.tensor.opt.MakeVector
   9.7%   100.0%       0.000s       1.23e-07s     C     2000       2   theano.tensor.elemwise.Sum
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
&lt;% time&gt; &lt;sum %&gt; &lt;apply time&gt; &lt;time per call&gt; &lt;type&gt; &lt;#call&gt; &lt;#apply&gt; &lt;Op name&gt;
  22.9%    22.9%       0.001s       2.91e-07s     C     2000        2   InplaceDimShuffle{x}
  14.1%    37.0%       0.000s       3.58e-07s     C     1000        1   MakeVector{dtype=&#39;float64&#39;}
  11.5%    48.5%       0.000s       2.93e-07s     C     1000        1   Elemwise{Composite{(i0 + (-sqr((i1 - ((i2 * i3) + i4)))))}}
  11.3%    59.8%       0.000s       2.88e-07s     C     1000        1   Elemwise{Composite{(i0 + (i1 * scalar_sigmoid(i2)))}}
  10.8%    70.6%       0.000s       2.75e-07s     C     1000        1   Elemwise{Composite{((i0 + Switch(Cast{int8}((GE((i1 + i2), i1) * LE((i1 + i2), i3))), i4, i5)) - ((i6 * scalar_softplus((-i7))) + i7))}}[(0, 2)]
   9.7%    80.3%       0.000s       2.47e-07s     C     1000        1   Elemwise{Composite{((i0 + Switch(Cast{int8}((GE(i1, i2) * LE(i1, i3))), i4, i5)) - ((i6 * scalar_softplus((-i7))) + i7))}}[(0, 1)]
   9.7%    90.0%       0.000s       1.23e-07s     C     2000        2   Sum{acc_dtype=float64}
   6.0%    96.0%       0.000s       1.52e-07s     C     1000        1   Elemwise{Composite{(i0 * scalar_sigmoid(i1))}}
   4.0%   100.0%       0.000s       1.03e-07s     C     1000        1   Elemwise{Mul}[(0, 1)]
   ... (remaining 0 Ops account for   0.00%(0.00s) of the runtime)

Apply
------
&lt;% time&gt; &lt;sum %&gt; &lt;apply time&gt; &lt;time per call&gt; &lt;#call&gt; &lt;id&gt; &lt;Apply name&gt;
  14.1%    14.1%       0.000s       3.58e-07s   1000     9   MakeVector{dtype=&#39;float64&#39;}(__logp_m_interval__, __logp_c_interval__, __logp_likelihood)
  13.5%    27.5%       0.000s       3.43e-07s   1000     3   InplaceDimShuffle{x}(m)
  11.5%    39.0%       0.000s       2.93e-07s   1000     4   Elemwise{Composite{(i0 + (-sqr((i1 - ((i2 * i3) + i4)))))}}(TensorConstant{(1,) of -1..0664093453}, TensorConstant{[13.059668...46571405]}, InplaceDimShuffle{x}.0, TensorConstant{[0. 1. 2. .. 7. 8. 9.]}, InplaceDimShuffle{x}.0)
  11.3%    50.4%       0.000s       2.88e-07s   1000     1   Elemwise{Composite{(i0 + (i1 * scalar_sigmoid(i2)))}}(TensorConstant{-10.0}, TensorConstant{20.0}, m_interval__)
  10.8%    61.2%       0.000s       2.75e-07s   1000     5   Elemwise{Composite{((i0 + Switch(Cast{int8}((GE((i1 + i2), i1) * LE((i1 + i2), i3))), i4, i5)) - ((i6 * scalar_softplus((-i7))) + i7))}}[(0, 2)](TensorConstant{2.995732273553991}, TensorConstant{-10.0}, Elemwise{Composite{(i0 * scalar_sigmoid(i1))}}.0, TensorConstant{10.0}, TensorConstant{-2.995732273553991}, TensorConstant{-inf}, TensorConstant{2.0}, c_interval__)
   9.7%    70.9%       0.000s       2.47e-07s   1000     6   Elemwise{Composite{((i0 + Switch(Cast{int8}((GE(i1, i2) * LE(i1, i3))), i4, i5)) - ((i6 * scalar_softplus((-i7))) + i7))}}[(0, 1)](TensorConstant{2.995732273553991}, m, TensorConstant{-10.0}, TensorConstant{10.0}, TensorConstant{-2.995732273553991}, TensorConstant{-inf}, TensorConstant{2.0}, m_interval__)
   9.4%    80.3%       0.000s       2.40e-07s   1000     2   InplaceDimShuffle{x}(Elemwise{Composite{(i0 * scalar_sigmoid(i1))}}.0)
   6.0%    86.3%       0.000s       1.52e-07s   1000     0   Elemwise{Composite{(i0 * scalar_sigmoid(i1))}}(TensorConstant{20.0}, c_interval__)
   5.2%    91.4%       0.000s       1.31e-07s   1000    10   Sum{acc_dtype=float64}(MakeVector{dtype=&#39;float64&#39;}.0)
   4.5%    96.0%       0.000s       1.15e-07s   1000     7   Sum{acc_dtype=float64}(Elemwise{Composite{(i0 + (-sqr((i1 - ((i2 * i3) + i4)))))}}.0)
   4.0%   100.0%       0.000s       1.03e-07s   1000     8   Elemwise{Mul}[(0, 1)](TensorConstant{0.5}, Sum{acc_dtype=float64}.0)
   ... (remaining 0 Apply instances account for 0.00%(0.00s) of the runtime)

Here are tips to potentially make your code run faster
                 (if you think of new ones, suggest them on the mailing list).
                 Test them first, as they are not guaranteed to always provide a speedup.
  - Try the Theano flag floatX=float32
  - Try installing amdlibm and set the Theano flag lib.amdlibm=True. This speeds up only some Elemwise operation.
</pre></div></div>
</div>
<div class="section" id="Authors">
<h2>Authors<a class="headerlink" href="#Authors" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>Adapted from a blog post by <a class="reference external" href="http://mattpitkin.github.io/samplers-demo/pages/pymc3-blackbox-likelihood/">Matt Pitkin</a> on August 27, 2018. That post was based on an example provided by <a class="reference external" href="https://github.com/jorgenem/">Jørgen Midtbø</a>.</li>
</ul>
</div>
</div>


    </div>
</div>
<div class="ui vertical footer segment">
    <div class="ui center aligned container">
        <a href="https://github.com/pymc-devs/pymc3"><i class="github icon large"></i></a>
        <a href="https://twitter.com/pymc_devs"><i class="twitter icon large"></i></a>
        <a href="https://discourse.pymc.io/"><i class="discourse icon large"></i></a>
    </div>
    <div class="ui center aligned container">
        <p>
            &copy; Copyright 2018, The PyMC Development Team.
        </p>
        <p>
            Created using <a href="https://sphinx-doc.org/">Sphinx</a> 1.8.3.<br />
        </p>
    </div>
</div>
  </body>
</html>