
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Sequential Monte Carlo &#8212; PyMC3 3.10.0 documentation</title>
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/semantic-sphinx.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/semantic-ui@2.4.2/dist/semantic.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/default.css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <script src="../_static/highlight.min.js"></script>
    <script src="../_static/semantic.min.js"></script>
    <link rel="shortcut icon" href="../_static/PyMC3.ico"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
<script>
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-176578023-1']);
  _gaq.push(['_trackPageview']);
</script>
<script>hljs.initHighlightingOnLoad();</script>
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">



  </head><body>
<div class="ui vertical center aligned">

    <div class="ui container">
        <div class="ui large secondary pointing menu">
            <a class="item" href="/">
                <img class="ui bottom aligned tiny image" src="https://cdn.rawgit.com/pymc-devs/pymc3/master/docs/logos/svg/PyMC3_banner.svg" />
            </a>
             <a href="../nb_tutorials/index.html" class="item">Tutorials</a> <a href="../nb_examples/index.html" class="item">Examples</a> <a href="../learn.html" class="item">Books + Videos</a> <a href="../api.html" class="item">API</a> <a href="../developer_guide.html" class="item">Developer Guide</a> <a href="../about.html" class="item">About PyMC3</a>
            
            <div class="right menu">
                <div class="item">
                    <form class="ui icon input" action="../search.html" method="get">
                        <input type="text" placeholder="Search..." name="q" />
                        <i class="search link icon"></i>
                    </form>
                </div>
                <a class="item" href="https://github.com/pymc-devs/pymc3"><i class="github blue icon large"></i></a>
            </div>
        </div>
    </div>
    
</div>

<div class="ui container" role="main">
    

    <div class="ui vertical segment">
        
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
</style>
<div class="section" id="Sequential-Monte-Carlo">
<h1>Sequential Monte Carlo<a class="headerlink" href="#Sequential-Monte-Carlo" title="Permalink to this headline">¶</a></h1>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pymc3</span> <span class="k">as</span> <span class="nn">pm</span>
<span class="kn">import</span> <span class="nn">theano.tensor</span> <span class="k">as</span> <span class="nn">tt</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Running on PyMC3 v</span><span class="si">{</span><span class="n">pm</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Running on PyMC3 v3.9.1
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">az</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">&quot;arviz-darkgrid&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Sampling from distributions with multiple peaks with standard MCMC methods can be difficult, if not impossible, as the Markov chain often gets stuck in either of the minima. A Sequential Monte Carlo sampler (SMC) is a way to ameliorate this problem.</p>
<p>As there are many SMC flavors, in this notebook we will focus on the version implemented in PyMC3.</p>
<p>SMC combines several statistical ideas, including <a class="reference external" href="https://en.wikipedia.org/wiki/Importance_sampling">importance sampling</a>, tempering and an MCMC kernel. By tempering we mean the use of an auxiliary <em>temperature</em> parameter to control the sampling process. This is easy to see if we write the posterior as:</p>
<div class="math notranslate nohighlight">
\[p(\theta \mid y)_{\beta} \propto p(y \mid \theta)^{\beta} \; p(\theta)\]</div>
<p>When <span class="math notranslate nohighlight">\(\beta=0\)</span> we have that <span class="math notranslate nohighlight">\(p(\theta \mid y)_{\beta=0}\)</span> is the prior distribution and when <span class="math notranslate nohighlight">\(\beta=1\)</span> we recover the <em>true</em> posterior. We can think of <span class="math notranslate nohighlight">\(\beta\)</span> as a knob to gradually turn-on the likelihood. This can be useful as in general sampling from the prior is easier than sampling from the posterior distribution. Thus we can use <span class="math notranslate nohighlight">\(\beta\)</span> to control the transition from an easy to sample distribution to a harder one.</p>
<p>A summary of the algorithm is:</p>
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(\beta\)</span> at zero and <code class="docutils literal notranslate"><span class="pre">stage</span></code> at zero.</p></li>
<li><p>Sample from the prior a set of samples <span class="math notranslate nohighlight">\(S_{\beta}\)</span> of size <span class="math notranslate nohighlight">\(N\)</span>. When <span class="math notranslate nohighlight">\(\beta = 0\)</span> the tempered posterior is the prior.</p></li>
<li><p>Increase <span class="math notranslate nohighlight">\(\beta\)</span> in order to make the effective sample size (ESS) equals some predefined value. We use <span class="math notranslate nohighlight">\(Nt\)</span>, where <span class="math notranslate nohighlight">\(t\)</span> is the threshold parameter – by default t=0.5. This means that the default ESS is fixed at half the number of draws.</p></li>
<li><p>Compute a set of <span class="math notranslate nohighlight">\(N\)</span> importance weights <span class="math notranslate nohighlight">\(W\)</span>. The weights are computed as the ratio of the tempered likelihoods at stage <span class="math notranslate nohighlight">\(i+1\)</span> and stage <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
<li><p>Obtain a new set of samples <span class="math notranslate nohighlight">\(S_{w}\)</span> by re-sampling <span class="math notranslate nohighlight">\(S_{\beta}\)</span> according to <span class="math notranslate nohighlight">\(W\)</span>.</p></li>
<li><p>Use the <span class="math notranslate nohighlight">\(S_{w}\)</span> to compute the covariance for (multivariate)normal proposal distribution.</p></li>
<li><p>For stages other than 0 use the acceptance rate from the previous stage to estimate the scaling of the proposal distribution and to compute <code class="docutils literal notranslate"><span class="pre">nsteps</span></code>.</p></li>
<li><p>Run <span class="math notranslate nohighlight">\(N\)</span> Metropolis chains (each one of length <code class="docutils literal notranslate"><span class="pre">n_steps</span></code>), starting each one from a different sample <span class="math notranslate nohighlight">\(S_{w}\)</span>.</p></li>
<li><p>Repeat from step 3 until <span class="math notranslate nohighlight">\(\beta \ge 1\)</span>.</p></li>
<li><p>The final result is a collection of <span class="math notranslate nohighlight">\(N\)</span> samples from the posterior.</p></li>
</ol>
<p>The algorithm is summarized in the next figure, the first subplot shows 5 samples (orange dots) at some particular stage. The second subplot shows how these samples are reweighted according to their posterior density (blue Gaussian curve). The third subplot shows the result of running a certain number of Metropolis steps, starting from the reweighted samples <span class="math notranslate nohighlight">\(S_{w}\)</span> in the second subplot, notice how the two samples with the lower posterior density (smaller circles) are discarded and not
used to seed Markov chains.</p>
<p><img alt="SMC stages" src="https://github.com/pymc-devs/pymc3/raw/master/docs/source/notebooks/smc.png" /></p>
<p>SMC samplers can also be interpreted in the light of genetic algorithms, which are biologically-inspired algorithms that can be summarized as follows:</p>
<ol class="arabic simple">
<li><p>Initialization: set a population of individuals</p></li>
<li><p>Mutation: individuals are somehow modified or perturbed</p></li>
<li><p>Selection: individuals with high <em>fitness</em> have higher chance to generate <em>offspring</em>.</p></li>
<li><p>Iterate by using individuals from 3 to set the population in 1.</p></li>
</ol>
<p>If each <em>individual</em> is a particular solution to a problem, then a genetic algorithm will eventually produce good solutions to that problem. One key aspect is to generate enough diversity (mutation step) in order to explore the solution space and hence avoiding getting trap in local minima. Then we perform a <em>selection</em> step to <em>probabilistically</em> keep reasonable solutions while also keeping some diversity. Being too greedy and short-sighted could be problematic, <em>bad</em> solutions in a given
moment could lead to <em>good</em> solutions in the future.</p>
<p>For the SMC version implemented in PyMC3 we set the number of parallel Markov chains <span class="math notranslate nohighlight">\(N\)</span> with the <code class="docutils literal notranslate"><span class="pre">draws</span></code> argument. At each stage SMC will use independent Markov chains to explore the <em>tempered posterior</em> (the black arrow in the figure). The final samples, <em>i.e</em> those stored in the <code class="docutils literal notranslate"><span class="pre">trace</span></code>, will be taken exclusively from the final stage (<span class="math notranslate nohighlight">\(\beta = 1\)</span>), i.e. the <em>true</em> posterior (“true” in the mathematical sense).</p>
<p>The successive values of <span class="math notranslate nohighlight">\(\beta\)</span> are determined automatically (step 3). The harder the distribution is to sample the closer two successive values of <span class="math notranslate nohighlight">\(\beta\)</span> will be. And the larger the number of stages SMC will take. SMC computes the next <span class="math notranslate nohighlight">\(\beta\)</span> value by keeping the effective sample size (ESS) between two stages at a constant predefined value of half the number of draws. This can be adjusted if necessary by the <code class="docutils literal notranslate"><span class="pre">threshold</span></code> parameter (in the interval [0, 1])– the current
default of 0.5 is generally considered as a good default. The larger this value, the higher the target ESS and the closer two successive values of <span class="math notranslate nohighlight">\(\beta\)</span> will be. This ESS values are computed from the importance weights (step 4) and not from the autocorrelation like those from ArviZ (for example using <code class="docutils literal notranslate"><span class="pre">az.ess</span></code> or <code class="docutils literal notranslate"><span class="pre">az.summary</span></code>).</p>
<p>Two more parameters that are automatically determined are:</p>
<ul class="simple">
<li><p>The number of steps each Markov chain takes to explore the <em>tempered posterior</em> <code class="docutils literal notranslate"><span class="pre">n_steps</span></code>. This is determined from the acceptance rate from the previous stage.</p></li>
<li><p>The (co)variance of the (Multivariate)Normal proposal distribution is also adjusted adaptively based on the acceptance rate at each stage.</p></li>
</ul>
<p>As with other sampling methods, running a sampler more than one time is useful to compute diagnostics, SMC is no exception. PyMC3 will try to run at least two <strong>SMC chains</strong> (do not confuse with the <span class="math notranslate nohighlight">\(N\)</span> Markov chains inside each SMC chain).</p>
<p>Even when SMC uses the Metropolis-Hasting algorithm under the hood, it has several advantages over it:</p>
<ul class="simple">
<li><p>It can sample from distributions with multiple peaks.</p></li>
<li><p>It does not have a burn-in period, it starts by sampling directly from the prior and then at each stage the starting points are already <em>approximately</em> distributed according to the tempered posterior (due to the re-weighting step).</p></li>
<li><p>It is inherently parallel (PyMC4 will take better advantage of this feature).</p></li>
</ul>
<div class="section" id="Solving-a-PyMC3-model-with-SMC">
<h2>Solving a PyMC3 model with SMC<a class="headerlink" href="#Solving-a-PyMC3-model-with-SMC" title="Permalink to this headline">¶</a></h2>
<p>To see an example of how to use SMC inside PyMC3 let’s define a multivariate Gaussian of dimension <span class="math notranslate nohighlight">\(n\)</span> with two modes, the weights of each mode and the covariance matrix.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">4</span>

<span class="n">mu1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">mu2</span> <span class="o">=</span> <span class="o">-</span><span class="n">mu1</span>

<span class="n">stdev</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">stdev</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">isigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">sigma</span><span class="p">)</span>
<span class="n">dsigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">sigma</span><span class="p">)</span>

<span class="n">w1</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># one mode with 0.1 of the mass</span>
<span class="n">w2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">w1</span>  <span class="c1"># the other mode with 0.9 of the mass</span>


<span class="k">def</span> <span class="nf">two_gaussians</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">log_like1</span> <span class="o">=</span> <span class="p">(</span>
        <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">n</span> <span class="o">*</span> <span class="n">tt</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>
        <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">tt</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">dsigma</span><span class="p">)</span>
        <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">isigma</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu1</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">log_like2</span> <span class="o">=</span> <span class="p">(</span>
        <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">n</span> <span class="o">*</span> <span class="n">tt</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>
        <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">tt</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">dsigma</span><span class="p">)</span>
        <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu2</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">isigma</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu2</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">pm</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">([</span><span class="n">tt</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">w1</span><span class="p">)</span> <span class="o">+</span> <span class="n">log_like1</span><span class="p">,</span> <span class="n">tt</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">w2</span><span class="p">)</span> <span class="o">+</span> <span class="n">log_like2</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span>
        <span class="s2">&quot;X&quot;</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">n</span><span class="p">,</span>
        <span class="n">lower</span><span class="o">=-</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">mu1</span><span class="p">),</span>
        <span class="n">upper</span><span class="o">=</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">mu1</span><span class="p">),</span>
        <span class="n">testval</span><span class="o">=-</span><span class="mf">1.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">mu1</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="n">llk</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Potential</span><span class="p">(</span><span class="s2">&quot;llk&quot;</span><span class="p">,</span> <span class="n">two_gaussians</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_smc</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="n">parallel</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">az_trace</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_pymc3</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Initializing SMC sampler...
Multiprocess sampling (2 chains in 2 jobs)
Stage:   0 Beta: 0.010
Stage:   1 Beta: 0.028
Stage:   2 Beta: 0.064
Stage:   3 Beta: 0.141
Stage:   4 Beta: 0.300
Stage:   5 Beta: 0.608
Stage:   6 Beta: 1.000
</pre></div></div>
</div>
<p>We can see from the message that PyMC3 is running two <strong>SMC chains</strong> in parallel. As explained before this is useful for diagnostics. As with other samplers one useful diagnostics is the <code class="docutils literal notranslate"><span class="pre">plot_trace</span></code>, here we use <code class="docutils literal notranslate"><span class="pre">kind=&quot;rank_vlines&quot;</span></code> as rank plots as generally more useful than the classical “trace”</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">az_trace</span><span class="p">,</span> <span class="n">compact</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s2">&quot;rank_vlines&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_SMC2_gaussians_8_0.png" src="../_images/notebooks_SMC2_gaussians_8_0.png" />
</div>
</div>
<p>From the KDE we can see that we recover the modes and even the relative weights seems pretty good. The rank plot on the right looks good too. One SMC chain is represented in blue and the other in orange. The vertical lines indicate deviation from the ideal expected value, which is represented with a black dashed line. If a vertical line is above the reference black dashed line we have more samples than expected, if the vertical line is below the sampler is getting less samples than expected.
Deviations like the ones in the figure above are fine and not a reason for concern.</p>
<p>As previously said SMC internally computes an estimation of the ESS (from importance weights). Those ESS values are not useful for diagnostics as they are a fixed target value.</p>
<p>We can compute ESS values from the trace returned by <code class="docutils literal notranslate"><span class="pre">sample_smc</span></code>, this is probably an overly optimistic value, as the computation of this ESS value takes autocorrelation into account and each SMC run/chain has low autocorrelation by construction.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_ess</span><span class="p">(</span><span class="n">az_trace</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_SMC2_gaussians_10_0.png" src="../_images/notebooks_SMC2_gaussians_10_0.png" />
</div>
</div>
</div>
<div class="section" id="Kill-your-darlings">
<h2>Kill your darlings<a class="headerlink" href="#Kill-your-darlings" title="Permalink to this headline">¶</a></h2>
<p>SMC (with a Metropolis kernel) is not free of problems, as it relies on Metropolis it will deteriorate as the dimensionality of the problem increases and/or if the geometry of the posterior is <em>weird</em> as in hierarchical models. To some extent increasing the number of draws and maybe the number of <code class="docutils literal notranslate"><span class="pre">n_steps</span></code> could help. To access the number of steps per stage you can check <code class="docutils literal notranslate"><span class="pre">trace.report.nsteps</span></code>. Ideally SMC will take a number of steps lower than <code class="docutils literal notranslate"><span class="pre">n_steps</span></code>.</p>
<p>Let’s make SMC fails spectacularly. We will run the same model as before, but increasing the dimensionality from 4 to 40.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">40</span>

<span class="n">mu1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">mu2</span> <span class="o">=</span> <span class="o">-</span><span class="n">mu1</span>

<span class="n">stdev</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">stdev</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">isigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">sigma</span><span class="p">)</span>
<span class="n">dsigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">sigma</span><span class="p">)</span>

<span class="n">w1</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">w2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">w1</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">two_gaussians</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">log_like1</span> <span class="o">=</span> <span class="p">(</span>
        <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">n</span> <span class="o">*</span> <span class="n">tt</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>
        <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">tt</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">dsigma</span><span class="p">)</span>
        <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">isigma</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu1</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">log_like2</span> <span class="o">=</span> <span class="p">(</span>
        <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">n</span> <span class="o">*</span> <span class="n">tt</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>
        <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">tt</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">dsigma</span><span class="p">)</span>
        <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu2</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">isigma</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu2</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">pm</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">([</span><span class="n">tt</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">w1</span><span class="p">)</span> <span class="o">+</span> <span class="n">log_like1</span><span class="p">,</span> <span class="n">tt</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">w2</span><span class="p">)</span> <span class="o">+</span> <span class="n">log_like2</span><span class="p">])</span>


<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span>
        <span class="s2">&quot;X&quot;</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">n</span><span class="p">,</span>
        <span class="n">lower</span><span class="o">=-</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">mu1</span><span class="p">),</span>
        <span class="n">upper</span><span class="o">=</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">mu1</span><span class="p">),</span>
        <span class="n">testval</span><span class="o">=-</span><span class="mf">1.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">mu1</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="n">llk</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Potential</span><span class="p">(</span><span class="s2">&quot;llk&quot;</span><span class="p">,</span> <span class="n">two_gaussians</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_smc</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="n">parallel</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">az_trace</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_pymc3</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Initializing SMC sampler...
Multiprocess sampling (2 chains in 2 jobs)
Stage:   0 Beta: 0.002
Stage:   1 Beta: 0.004
Stage:   2 Beta: 0.006
Stage:   3 Beta: 0.009
Stage:   4 Beta: 0.012
Stage:   5 Beta: 0.017
Stage:   6 Beta: 0.021
Stage:   7 Beta: 0.027
Stage:   8 Beta: 0.034
Stage:   9 Beta: 0.042
Stage:  10 Beta: 0.052
Stage:  11 Beta: 0.064
Stage:  12 Beta: 0.078
Stage:  13 Beta: 0.097
Stage:  14 Beta: 0.122
Stage:  15 Beta: 0.152
Stage:  16 Beta: 0.191
Stage:  17 Beta: 0.240
Stage:  18 Beta: 0.301
Stage:  19 Beta: 0.373
Stage:  20 Beta: 0.462
Stage:  21 Beta: 0.566
Stage:  22 Beta: 0.693
Stage:  23 Beta: 0.854
Stage:  24 Beta: 1.000
</pre></div></div>
</div>
<p>We see that SMC recognizes this is a harder problem and increases the number of stages. Unfortunately in this case that is not enough to recover the correct posterior as we can see in the following plot.</p>
<p>Compare this rank plot with the one obtained in the previous example (n=4). The rank plot is telling us that the <em>blue chain</em> is sampling an excess of low parameter values (ranks below 2000) and is failing to sample from high parameter values. The orange-chain is doing the exact opposite. So basically one SMC chain is exploring one mode and the other SMC chain the other, they are failing to mix and to recover the relative weights of each mode.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">az_trace</span><span class="p">,</span> <span class="n">compact</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s2">&quot;rank_vlines&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_SMC2_gaussians_15_0.png" src="../_images/notebooks_SMC2_gaussians_15_0.png" />
</div>
</div>
<p>Even when each SMC run has low autocorrelation by construction the ESS value computed by ArviZ may under some circumstances show problems. This example is such a case, here each SMC chain is basically exploring a single mode and missing the other. When this happens the value of ESS_bulk (computed using <code class="docutils literal notranslate"><span class="pre">az.summary</span></code> or <code class="docutils literal notranslate"><span class="pre">az.ess</span></code>) will be close to the number of modes, for this problem we got ~3. We are working on providing an ESS estimation better tailored to the peculiarities of SMC.</p>
<p>As the ESS value may vary across the parameter space. We also recommend to check the behavior of localized version of ESS. We can do this with <code class="docutils literal notranslate"><span class="pre">plot_ess</span></code>. The recommended value for ESS is at least 400 (dashed gray line).</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="c1"># only the first 6 dimensions</span>
<span class="n">az</span><span class="o">.</span><span class="n">plot_ess</span><span class="p">(</span><span class="n">az_trace</span><span class="p">,</span> <span class="n">coords</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;X_dim_0&quot;</span><span class="p">:</span> <span class="nb">slice</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)});</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_SMC2_gaussians_17_0.png" src="../_images/notebooks_SMC2_gaussians_17_0.png" />
</div>
</div>
<p>You may want to repeat the SMC sampling with the failing model as you may get different problems each time.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">load_ext</span> watermark
<span class="o">%</span><span class="k">watermark</span> -n -u -v -iv -w
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
arviz    0.8.3
numpy    1.18.1
autopep8 1.5
json     2.0.9
pymc3    3.9.1
last updated: Mon Jun 29 2020

CPython 3.7.6
IPython 7.12.0
watermark 2.0.2
</pre></div></div>
</div>
</div>
</div>


    </div>
</div>
<div class="ui vertical footer segment">
    <div class="ui center aligned container">
        <a href="https://github.com/pymc-devs/pymc3"><i class="github icon large"></i></a>
        <a href="https://twitter.com/pymc_devs"><i class="twitter icon large"></i></a>
        <a href="https://discourse.pymc.io/"><i class="discourse icon large"></i></a>
    </div>
    <div class="ui center aligned container">This page uses <a href="https://analytics.google.com/">
    Google Analytics</a> to collect statistics. You can disable it by blocking
    the JavaScript coming from www.google-analytics.com.
    <script>
      (function() {
        var ga = document.createElement('script');
        ga.src = ('https:' == document.location.protocol ?
                  'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        ga.setAttribute('async', 'true');
        document.documentElement.firstChild.appendChild(ga);
      })();
    </script>
    </div>
    <div class="ui center aligned container">
        <p>
            &copy; Copyright 2018, The PyMC Development Team.
        </p>
        <p>
            Created using <a href="https://sphinx-doc.org/">Sphinx</a> 3.4.0.<br />
        </p>
    </div>
</div>
  </body>
</html>